{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72eb777-2427-48b4-a519-9a57273d172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import requests\n",
    "from scipy.sparse import csr_matrix\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4cc52d-34d5-493c-8e39-0964285dae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12ca1ae-832d-4eb6-bec1-e70d7f4b9d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "pythia = AutoModel.from_pretrained('EleutherAI/pythia-70m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04118122-24c9-457e-8b8e-e2781827ebbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoModel.from_pretrained("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cc3d5-8bc5-4e5e-a06e-9788f79ef95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf Interpreting-Reward-Models || true\n",
    "! git clone https://github.com/apartresearch/Interpreting-Reward-Models.git\n",
    "! cd Interpreting-Reward-Models && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e39d1e-c04a-4100-8577-fb4e923777f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_analyzer import SparseAutoencoder, TaskConfig\n",
    "from reward_analyzer.utils.model_storage_utils import load_autoencoders_for_artifact, load_latest_model_from_hub\n",
    "from reward_analyzer.utils.transformer_utils import batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe6b31-45e6-49b7-bed8-880ee829f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'EleutherAI/gpt-neo-125m'\n",
    "# model_name = 'EleutherAI/pythia-70m'\n",
    "# model_name = 'EleutherAI/pythia-160m'\n",
    "\n",
    "task = TaskConfig.HH_RLHF\n",
    "task_name = task.name\n",
    "version = 'v0'\n",
    "\n",
    "if 'pythia' in model_name:\n",
    "    layer_name_step = 'layers.{}.mlp'\n",
    "elif 'neo' in model_name:\n",
    "    layer_name_stem = 'h.{}.mlp'\n",
    "elif 'gemma' in model_name:\n",
    "    layer_name_stem = 'layers.{}.mlp'\n",
    "else:\n",
    "    raise Exception(f'Not familiar with model name family of {model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb831298-a97a-4d7c-b215-9cbd8255c181",
   "metadata": {},
   "source": [
    "### Load model and autoencoder artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78e2e80-8efd-4d44-9572-ab3c3cdfe7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer =  AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "model = load_latest_model_from_hub(model_name = model_name, task_config=task)\n",
    "model.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36957a-00dc-4b57-a237-817306b7e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoders_dict = load_autoencoders_for_artifact(f'nlp_and_interpretability/Autoencoder_training_hh_rlhf/autoencoders_{model_name.split(\"/\")[-1].replace(\"-\", \"_\")}_{task_name}:{version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc62c1a-59ac-4a78-b3b2-85e601004028",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf_small = autoencoders_dict['rlhf_small']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f08b322-2a22-4018-9f26-6a0479e924af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_data_to_jsonl(data: dict, filename: str):\n",
    "    list_lengths = [len(value_list) for value_list in data.values()]\n",
    "\n",
    "    assert min(list_lengths) == max(list_lengths), f'Expected list lengths to be the same! Instead got {list_lengths}'\n",
    "    n = max(list_lengths)\n",
    "    print(f'Writing to file name now')\n",
    "\n",
    "\n",
    "    # Open a file to write JSON Lines\n",
    "    with open(filename, 'w') as jsonl_file:\n",
    "        # Iterate over the index of the lists\n",
    "        for i in range(n):\n",
    "            # Create a dictionary for the current JSON object\n",
    "            json_object = {key: values[i] for key, values in data.items()}\n",
    "            # Write the JSON object as a line in the JSONL file\n",
    "            jsonl_file.write(json.dumps(json_object) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e195a-56dc-479c-ba2e-a8ec885c6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_from_single_input(single_input):\n",
    "    return torch.mean(single_input, dim=0)\n",
    "\n",
    "def extract_and_process_activations(texts, model, tokenizer, layer_name_stem, autoencoders_dict):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "    token_ids = inputs[\"input_ids\"].squeeze().tolist()\n",
    "    activations = {}\n",
    "\n",
    "    target_layer_names = [layer_name_stem.format(key) for key in autoencoders_dict]\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    hooks = [\n",
    "        module.register_forward_hook(get_activation(name))\n",
    "        for name, module in model.named_modules()\n",
    "        if name in target_layer_names\n",
    "    ]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    specified_activations =  {layer_num: activations[layer_name_stem.format(layer_num)] for layer_num in autoencoders_dict}\n",
    "    final_token_embeddings = outputs.last_hidden_state.squeeze().detach().tolist()\n",
    "    final_token_embeddings = [item[0] for item in final_token_embeddings]\n",
    "\n",
    "\n",
    "    all_features = {\n",
    "        \"texts\": texts,\n",
    "        \"token_ids\": token_ids,\n",
    "        \"token_embeddings\": final_token_embeddings\n",
    "    }\n",
    "\n",
    "    for layer_num, activation_values in specified_activations.items():\n",
    "        activation_values = activation_values.squeeze(0).cpu()\n",
    "        autoencoder = autoencoders_dict[layer_num]\n",
    "        batch_features, _ = autoencoder(activation_values)\n",
    "        batch_features = batch_features.detach().squeeze(0)\n",
    "\n",
    "        all_features[f'activations_{layer_num}'] = activation_values.detach().cpu().squeeze(0).numpy().tolist()\n",
    "\n",
    "        full_reprs = []\n",
    "        averaged_reprs = []\n",
    "        for single_feature in batch_features:\n",
    "            averaged_repr_each_input = features_from_single_input(single_feature).cpu().tolist()\n",
    "\n",
    "            full_reprs.append(single_feature.cpu().tolist())\n",
    "            averaged_reprs.append(averaged_repr_each_input)\n",
    "\n",
    "        all_features[f'full_repr_{layer_num}'] = full_reprs\n",
    "        all_features[f'averaged_reprs_{layer_num}'] = averaged_reprs\n",
    "\n",
    "    return all_features\n",
    "\n",
    "\n",
    "def extract_features_batched(texts, model, tokenizer, layer_name_stem, autoencoders_dict, output_file=None, batch_size=8):\n",
    "    output_file = output_file or f'./{model_name}_{task.name}_activations_dataset.jsonl'.split(\"/\")[-1].replace(\"-\", \"_\")\n",
    "    for curr_batch in tqdm(batch(texts, n=batch_size)):\n",
    "        features = extract_and_process_activations(texts, model, tokenizer, layer_name_stem, autoencoders_dict)\n",
    "        dump_data_to_jsonl(features, filename = output_file)\n",
    "\n",
    "    return features\n",
    "\n",
    "extract_features_batched(texts=50*texts, model=model, tokenizer=tokenizer, layer_name_stem=layer_name_stem, autoencoders_dict=rlhf_small, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2539b65-6e03-4a31-82c1-86232aa7a2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"gpt-neo-125m_hh_rlhf_activations_dataset.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d871f1-f4a5-4e5f-8f2f-45f081ce51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_dataset_to_wandb(training_dataset: Dataset, model_name, dataset_name=\"logistic_probe_data.hf\"):\n",
    "    out_filename = training_dataset.save_to_disk(dataset_name)\n",
    "    \n",
    "    my_artifact = wandb.Artifact(f\"logistic_probe_training_dataset_{model_name}\", type=\"data\")\n",
    "    \n",
    "    # Add the list to the artifact\n",
    "    my_artifact.add_file(local_path=out_filename, name=\"logistic_probe_training_dataset\")\n",
    "\n",
    "    metadata_dict = {\n",
    "        \"description\": \"Training dataset, with activations and rewards\",\n",
    "        \"source\": \"Generated by my script\",\n",
    "        \"num_examples\": len(training_dataset),\n",
    "        \"split\": \"full\"\n",
    "    }\n",
    "\n",
    "    my_artifact.metadata.update(metadata_dict)\n",
    "\n",
    "    # Log the artifact to the run\n",
    "    wandb.log_artifact(my_artifact)\n",
    "\n",
    "save_training_dataset_to_wandb(full_training_dataset, model_name=model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05547619-a45a-4aa1-9c3c-5df6fef14dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
