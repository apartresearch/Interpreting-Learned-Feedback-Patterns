{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72eb777-2427-48b4-a519-9a57273d172c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35cc3d5-8bc5-4e5e-a06e-9788f79ef95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! rm -rf Interpreting-Reward-Models || true\n",
    "! git clone https://github.com/apartresearch/Interpreting-Reward-Models.git\n",
    "! cd Interpreting-Reward-Models && pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e39d1e-c04a-4100-8577-fb4e923777f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reward_analyzer import SparseAutoencoder\n",
    "from reward_analyzer.utils.model_storage_utils import load_autoencoders_for_artifact\n",
    "from reward_analyzer.utils.transformer_utils import batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe6b31-45e6-49b7-bed8-880ee829f8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt_neo_125m'\n",
    "task_name = 'hh_rlhf'\n",
    "version = 'v0'\n",
    "\n",
    "if 'pythia' in model_name:\n",
    "    layer_name_step = 'layers.{}.mlp'\n",
    "elif 'neo' in model_name:\n",
    "    layer_name_stem = 'h.{}.mlp'\n",
    "elif 'gemma' in model_name:\n",
    "    layer_name_stem = 'layers.{}.mlp'\n",
    "else:\n",
    "    raise Exception(f'Not familiar with model name family of {model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a36957a-00dc-4b57-a237-817306b7e5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoders_dict = load_autoencoders_for_artifact(f'nlp_and_interpretability/Autoencoder_training_hh_rlhf/autoencoders_{model_name}_{task_name}:{version}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc62c1a-59ac-4a78-b3b2-85e601004028",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf_small = autoencoders_dict['rlhf_small']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023e195a-56dc-479c-ba2e-a8ec885c6c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_process_activations(texts, model, tokenizer, layer_name_stem, autoencoders_dict):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    token_ids = inputs[\"input_ids\"].squeeze().tolist()\n",
    "    activations = {}\n",
    "\n",
    "    target_layer_names = [layer_name_stem.format(key) for key in autoencoders_dict.keys()]\n",
    "\n",
    "    def get_activation(name):\n",
    "        def hook(model, input, output):\n",
    "            activations[name] = output.detach()\n",
    "        return hook\n",
    "\n",
    "    hooks = [\n",
    "        module.register_forward_hook(get_activation(name))\n",
    "        for name, module in model.named_modules()\n",
    "        if name in target_layer_names\n",
    "    ]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        model(**inputs)\n",
    "\n",
    "    for hook in hooks:\n",
    "        hook.remove()\n",
    "\n",
    "    specified_activations = [(name, activations[name]) for name in target_layer_names]\n",
    "    concatenated_activations = [[] for _ in token_ids]\n",
    "\n",
    "    for act, autoencoder_idx in zip(specified_activations, autoencoder_indices):\n",
    "        name, act = act\n",
    "        act = act.squeeze(0)\n",
    "        autoencoder = autoencoders[autoencoder_idx]\n",
    "        features, _ = autoencoder(act)\n",
    "        for i in range(len(token_ids)):\n",
    "            concatenated_activations[i].append(features[i].tolist())\n",
    "\n",
    "    final_activations = {\n",
    "        token_id: [item for sublist in concatenated_activations[i] for item in sublist]\n",
    "        for i, token_id in enumerate(token_ids)\n",
    "    }\n",
    "\n",
    "    return final_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d871f1-f4a5-4e5f-8f2f-45f081ce51ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_dataset_to_wandb(training_dataset: Dataset, model_name, dataset_name=\"logistic_probe_data.hf\"):\n",
    "    out_filename = training_dataset.save_to_disk(dataset_name)\n",
    "    \n",
    "    my_artifact = wandb.Artifact(f\"logistic_probe_training_dataset_{model_name}\", type=\"data\")\n",
    "    \n",
    "    # Add the list to the artifact\n",
    "    my_artifact.add_file(local_path=out_filename, name=\"logistic_probe_training_dataset\")\n",
    "\n",
    "    metadata_dict = {\n",
    "        \"description\": \"Training dataset, with activations and rewards\",\n",
    "        \"source\": \"Generated by my script\",\n",
    "        \"num_examples\": len(training_dataset),\n",
    "        \"split\": \"full\"\n",
    "    }\n",
    "\n",
    "    my_artifact.metadata.update(metadata_dict)\n",
    "\n",
    "    # Log the artifact to the run\n",
    "    wandb.log_artifact(my_artifact)\n",
    "\n",
    "save_training_dataset_to_wandb(full_training_dataset, model_name=model_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
