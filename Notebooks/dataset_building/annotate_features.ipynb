{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb482346-b84e-4b5e-b182-b825650ec27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "import os\n",
    "import json\n",
    "from IPython.display import HTML\n",
    "\n",
    "# Initialize the wandb run\n",
    "run = wandb.init()\n",
    "\n",
    "# Access the artifact and download it\n",
    "artifact = run.use_artifact('nlp_and_interpretability/feature_explanations/pythia-70m_hh_rlhf_explanations_artifact:v2', type='dataset')\n",
    "artifact_dir = artifact.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c957a9-d314-4e18-bf84-3894ec78a246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locate the JSON file in the downloaded directory\n",
    "# Assuming there's only one JSON file, or you know the filename beforehand\n",
    "json_file_path = None\n",
    "for file_name in os.listdir(artifact_dir):\n",
    "    if file_name.endswith(\".json\"):\n",
    "        json_file_path = os.path.join(artifact_dir, file_name)\n",
    "        break\n",
    "\n",
    "# Load the JSON file\n",
    "if json_file_path:\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        data = json.load(json_file)\n",
    "        # Display or process the JSON data as needed\n",
    "else:\n",
    "    print(\"No JSON file found in the artifact directory.\")\n",
    "#data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b57a90f-9e56-4e0b-8608-553f5d983a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"I\\t0.0\\nĠread\\t0.0\\nĠit\\t0.0\\nĠsomewhere\\t0.0\\nĠand\\t0.0\\nĠit\\t1.0\\nĠwould\\t2.0\\nĠmake\\t0.0\\nI\\t0.0\\nĠhave\\t0.0\\nĠno\\t0.0\\nĠidea\\t2.0\\nĠwhen\\t0.0\\nĠmy\\t0.0\\nĠperiod\\t0.0\\nĠshould\\t1.0\\nI\\t0.0\\nĠreally\\t0.0\\nĠdon\\t0.0\\n't\\t0.0\\nĠknow\\t2.0\\nĠwhere\\t1.0\\nĠthis\\t0.0\\nĠmonth\\t0.0\\nNot\\t0.0\\nĠsure\\t2.0\\nĠif\\t1.0\\nĠI\\t0.0\\nĠmentioned\\t0.0\\nĠthis\\t0.0\\nĠbefore\\t0.0\\nĠbut\\t0.0\\nNot\\t0.0\\nĠsure\\t2.0\\nĠif\\t1.0\\nĠI\\t0.0\\nâĢĻ\\t0.0\\nve\\t0.0\\nĠshared\\t0.0\\nĠthis\\t0.0\"\n",
    "\n",
    "def parse_input(input_text):\n",
    "    output = []\n",
    "    for word_and_activation in input_text.strip().split('\\n'):\n",
    "        token, activation = word_and_activation.split('\\t')\n",
    "        token = token.replace('Ġ', '')\n",
    "        output.append((token, float(activation)))\n",
    "    return output\n",
    "\n",
    "parse_input(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656ad4b0-bc4e-4c94-b444-c63bd1d3b9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuronExplanation:\n",
    "\n",
    "    def __init__(self, neuron_name, tokens_and_activations, explanation):\n",
    "        self.neuron_name = neuron_name\n",
    "        self.tokens_and_activations = tokens_and_activations.copy()\n",
    "        self.explanation = explanation\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'Neuron_name: {self.neuron_name},\\nTokens_and_activations: {self.tokens_and_activations},\\nExplanation: {self.explanation}'\n",
    "\n",
    "all_neuron_explanations = []\n",
    "for neuron_name, explanation_and_activations in data.items():\n",
    "    explanation = explanation_and_activations['explanation']\n",
    "    original_activations = explanation_and_activations['original_activations']\n",
    "    tokens_and_activations = parse_input(original_activations)\n",
    "\n",
    "    neuron_explanation = NeuronExplanation(\n",
    "        neuron_name=neuron_name, tokens_and_activations=tokens_and_activations, explanation=explanation)\n",
    "\n",
    "    all_neuron_explanations.append(neuron_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e332cf1c-f854-400f-b615-6b3206b52489",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = [len(item.tokens_and_activations) for item in all_neuron_explanations]\n",
    "#lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f8ac4e-e6ae-43db-ab6b-b44aebe34282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from textwrap import dedent\n",
    "\n",
    "class TestQuestion:\n",
    "    def __init__(self, true_explanation, scrambled_explanations, tokens_and_activations):\n",
    "        self.true_explanation = true_explanation\n",
    "        self.scrambled_explanations = scrambled_explanations\n",
    "        self.tokens_and_activations = tokens_and_activations\n",
    "        self.markdown = self.prepare_markdown(self.tokens_and_activations)\n",
    "        self.max_options = 4\n",
    "        \n",
    "        self.options = [self.true_explanation] + self.scrambled_explanations\n",
    "        if len(self.options) < self.max_options:\n",
    "            self.options += (self.max_options - len(self.options)) * [\"Padding option - please ignore\"]\n",
    "        random.shuffle(self.options)\n",
    "        self.true_option = self.options.index(true_explanation)\n",
    "        \n",
    "        self.prompt = (f\"\"\"Below is a sequence of texts, alongwith the activations for a neuron.\n",
    "Red being the darkest. Pick which of the explanations below is the best choice, or none of them\n",
    "above if none seem suitable.<br>\n",
    "Token Activations:\n",
    "<br>{self.markdown}\n",
    "<br><br>\n",
    "Explanation 1: {self.options[0]}\n",
    "<br><br>\n",
    "Explanation 2: {self.options[1]}\n",
    "<br><br>\n",
    "Explanation 3: {self.options[2]}\n",
    "<br><br>\n",
    "Explanation 4: {self.options[3]}\n",
    "<br><br>\n",
    "Explanation 5: None of the above.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "    def to_json(self):\n",
    "        result = {\n",
    "            \"prompt\": self.prompt,\n",
    "            \"options\": self.options,\n",
    "            \"true_option\": self.true_option\n",
    "        }\n",
    "        return result\n",
    "\n",
    "    def __str__(self):\n",
    "        return f'True explanation: {self.true_explanation} \\n {len(self.tokens_and_activations)}'\n",
    "\n",
    "    def activation_to_color(self, activation, min_activation, max_activation):\n",
    "        # Normalize the activation value to [0, 1]\n",
    "        if max_activation - min_activation == 0:  # Avoid division by zero\n",
    "            return 'rgba(0, 255, 0, 0)'  # Fully transparent if no range\n",
    "        \n",
    "        normalized = (activation - min_activation) / (max_activation - min_activation)\n",
    "        red_value = int(normalized * 255)  # Scale to 255 for green\n",
    "        return f'rgba({red_value}, 0, 0, {normalized})'  # RGB color from transparent to green\n",
    "\n",
    "    \n",
    "    def prepare_markdown(self, tokens_and_activations):\n",
    "        min_activation = min(word_activation[1] for word_activation in tokens_and_activations)\n",
    "        max_activation = max(word_activation[1] for word_activation in tokens_and_activations)\n",
    "    \n",
    "        # Build HTML string with colored activations\n",
    "        html_output = \"\"\n",
    "        for word, activation in tokens_and_activations:\n",
    "            color = self.activation_to_color(activation, min_activation, max_activation)\n",
    "            html_output += f'<span style=\"background-color: {color}; padding: 2px;\">{word}</span> '\n",
    "    \n",
    "        return html_output\n",
    "\n",
    "def generate_test_questions(all_neuron_explanations, k=3):\n",
    "    test_questions = []\n",
    "    all_explanations = [ne.explanation for ne in all_neuron_explanations]\n",
    "    \n",
    "    for ne in all_neuron_explanations:\n",
    "        true_explanation = ne.explanation\n",
    "        tokens_and_activations = ne.tokens_and_activations\n",
    "\n",
    "        # Generate a scrambled list of other explanations (excluding the true explanation)\n",
    "        scrambled_explanations = random.sample(\n",
    "            [exp for exp in all_explanations if exp != true_explanation],\n",
    "            k=min(k, len(all_explanations) - 1)\n",
    "        )\n",
    "        # Create a new TestQuestion\n",
    "        test_question = TestQuestion(\n",
    "            true_explanation=true_explanation,\n",
    "            scrambled_explanations=scrambled_explanations,\n",
    "            tokens_and_activations=tokens_and_activations\n",
    "        )\n",
    "\n",
    "        # Add to the list\n",
    "        test_questions.append(test_question)\n",
    "\n",
    "    return test_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b02b6a-1af6-46e0-8af1-d705a38c2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_questions = generate_test_questions(all_neuron_explanations, k=3)\n",
    "all_json = [question.to_json() for question in test_questions]\n",
    "\n",
    "import json\n",
    "\n",
    "with open(\"annotation_dataset.json\", \"w\") as f_out:\n",
    "    json.dump(all_json, f_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86387452-04c3-4bf3-9465-ea10979458c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "item_no = 73\n",
    "html_output = test_questions[item_no].prompt\n",
    "print(test_questions[item_no].true_option)\n",
    "print(test_questions[item_no].true_explanation)\n",
    "HTML(html_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e02c325-b002-46e9-813b-4de614427fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "HTML(test_questions[25].markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af77f6b0-6a5d-40ad-91b7-e74cd7d71018",
   "metadata": {},
   "source": [
    "### Parse annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4e1b23-7e33-4b31-9ba5-24fab82d019d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"final_annotations.json\", \"r\") as f_in:\n",
    "    all_annotations = json.load(f_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcee1a2c-6195-4b89-83e2-dcc132c2c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_labels = []\n",
    "annotator_labels = {1: [], 2: []}\n",
    "\n",
    "def calculate_agreement_rate(ground_truth, labels):\n",
    "    # Ensure both lists are the same length\n",
    "    if len(ground_truth) != len(labels):\n",
    "        raise ValueError(\"Both lists must be of the same length\")\n",
    "\n",
    "    # Calculate the number of agreements\n",
    "    agreements = sum(1 for gt, label in zip(ground_truth, labels) if gt == label)\n",
    "    \n",
    "    # Calculate the agreement rate\n",
    "    agreement_rate = agreements / len(ground_truth)\n",
    "    \n",
    "    return agreement_rate\n",
    "\n",
    "\n",
    "def process_annotations(annotations, num_annotations=40):\n",
    "    correct_count = 0\n",
    "    for annotation_element in all_annotations[:num_annotations]:\n",
    "        annotations = annotation_element['annotations']\n",
    "        data = annotation_element['data']\n",
    "        true_labels.append(int(data['true_option']) + 1)\n",
    "        if len(annotations) >= 2:\n",
    "            correct_count+=1\n",
    "        for curr_annotation in annotations:\n",
    "            target = curr_annotation['completed_by']\n",
    "            annotator_label = int(curr_annotation['result'][0]['value']['choices'][0])\n",
    "            annotator_labels[target].append(annotator_label)\n",
    "\n",
    "process_annotations(all_annotations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3e0e54-5eb8-4b54-a617-406325655b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "none_of_the_above = {}\n",
    "\n",
    "for target in annotator_labels:\n",
    "    curr_labels = annotator_labels[target]\n",
    "    none_of_the_above[target] = len([item for item in curr_labels if item == 5])/len(curr_labels)\n",
    "    agreement_rate = calculate_agreement_rate(true_labels, curr_labels)\n",
    "    print(f'Agreement rate is {agreement_rate} for target {target}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95684da3-2edb-4b53-b845-0395eb29f989",
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_agreement_rate(annotator_labels[1], annotator_labels[2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
