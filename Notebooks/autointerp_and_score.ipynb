{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TWYBEIdoTPRq"
      },
      "source": [
        "# Sparse Coding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHuukYmU123X"
      },
      "source": [
        "##Imports and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRcnHszt12Aq",
        "outputId": "89a47abe-bf75-4b50-a8f7-f8bc6d1c6963"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers==4.34.0\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m27.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (0.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (2.31.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0)\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m59.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (0.4.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.34.0) (4.5.0)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0)\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.34.0) (2023.7.22)\n",
            "Installing collected packages: huggingface-hub, tokenizers, transformers\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.19.4\n",
            "    Uninstalling huggingface-hub-0.19.4:\n",
            "      Successfully uninstalled huggingface-hub-0.19.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.15.0\n",
            "    Uninstalling tokenizers-0.15.0:\n",
            "      Successfully uninstalled tokenizers-0.15.0\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.35.2\n",
            "    Uninstalling transformers-4.35.2:\n",
            "      Successfully uninstalled transformers-4.35.2\n",
            "Successfully installed huggingface-hub-0.17.3 tokenizers-0.14.1 transformers-4.34.0\n",
            "Collecting datasets==2.14.5\n",
            "  Downloading datasets-2.14.5-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.6/519.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets==2.14.5)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (3.4.1)\n",
            "Collecting multiprocess (from datasets==2.14.5)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (3.8.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (0.17.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.14.5) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (3.3.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.14.5) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets==2.14.5) (4.5.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.14.5) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.14.5) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets==2.14.5) (1.16.0)\n",
            "Installing collected packages: dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.5 dill-0.3.7 multiprocess-0.70.15\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement torch==2.0.1+cu118 (from versions: 1.11.0, 1.12.0, 1.12.1, 1.13.0, 1.13.1, 2.0.0, 2.0.1, 2.1.0, 2.1.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torch==2.0.1+cu118\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting einops==0.7.0\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n",
            "Collecting circuitsvis==1.41.0\n",
            "  Downloading circuitsvis-1.41.0-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata<6.0.0,>=5.1.0 (from circuitsvis==1.41.0)\n",
            "  Downloading importlib_metadata-5.2.0-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: numpy<2.0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from circuitsvis==1.41.0) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.10 in /usr/local/lib/python3.10/dist-packages (from circuitsvis==1.41.0) (2.1.0+cu118)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata<6.0.0,>=5.1.0->circuitsvis==1.41.0) (3.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis==1.41.0) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis==1.41.0) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis==1.41.0) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis==1.41.0) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis==1.41.0) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis==1.41.0) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10->circuitsvis==1.41.0) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10->circuitsvis==1.41.0) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10->circuitsvis==1.41.0) (1.3.0)\n",
            "Installing collected packages: importlib-metadata, circuitsvis\n",
            "  Attempting uninstall: importlib-metadata\n",
            "    Found existing installation: importlib-metadata 6.8.0\n",
            "    Uninstalling importlib-metadata-6.8.0:\n",
            "      Successfully uninstalled importlib-metadata-6.8.0\n",
            "Successfully installed circuitsvis-1.41.0 importlib-metadata-5.2.0\n",
            "Collecting openai==0.28.1\n",
            "  Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (4.66.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from openai==0.28.1) (3.8.6)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.20->openai==0.28.1) (2023.7.22)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (23.1.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->openai==0.28.1) (1.3.1)\n",
            "Installing collected packages: openai\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed openai-0.28.1\n",
            "Collecting wandb==0.15.12\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Click!=8.0.0,>=7.1 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.12) (8.1.7)\n",
            "Collecting GitPython!=3.1.29,>=1.0.0 (from wandb==0.15.12)\n",
            "  Downloading GitPython-3.1.40-py3-none-any.whl (190 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.6/190.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.12) (2.31.0)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.12) (5.9.5)\n",
            "Collecting sentry-sdk>=1.0.0 (from wandb==0.15.12)\n",
            "  Downloading sentry_sdk-1.36.0-py2.py3-none-any.whl (249 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m249.2/249.2 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb==0.15.12)\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.12) (6.0.1)\n",
            "Collecting pathtools (from wandb==0.15.12)\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb==0.15.12)\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.12) (67.7.2)\n",
            "Requirement already satisfied: appdirs>=1.4.3 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.12) (1.4.4)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.12) (3.20.3)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.15.12) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.12)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.15.12) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.15.12) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.15.12) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.0.0->wandb==0.15.12) (2023.7.22)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.12)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Building wheels for collected packages: pathtools\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8790 sha256=dd3c41f74bd782cd63b74581cef8f56e42689b0499590769ad7493687ae45045\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "Successfully built pathtools\n",
            "Installing collected packages: pathtools, smmap, setproctitle, sentry-sdk, docker-pycreds, gitdb, GitPython, wandb\n",
            "Successfully installed GitPython-3.1.40 docker-pycreds-0.4.0 gitdb-4.0.11 pathtools-0.1.2 sentry-sdk-1.36.0 setproctitle-1.3.3 smmap-5.0.1 wandb-0.15.12\n",
            "Requirement already satisfied: tqdm==4.66.1 in /usr/local/lib/python3.10/dist-packages (4.66.1)\n",
            "Requirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers==4.34.0\n",
        "!pip install datasets==2.14.5\n",
        "!pip install torch=='2.0.1+cu118'\n",
        "!pip install einops==0.7.0\n",
        "!pip install circuitsvis==1.41.0\n",
        "!pip install openai==0.28.1\n",
        "!pip install wandb==0.15.12\n",
        "!pip install tqdm==4.66.1\n",
        "!pip install nltk==3.8.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "S1jJ0lBi2CFB"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModel, AutoTokenizer, pipeline, AutoModelForCausalLM\n",
        "from transformers import AutoTokenizer\n",
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from collections import defaultdict\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "from scipy.stats import pearsonr\n",
        "import matplotlib.pyplot as plt\n",
        "from einops import rearrange\n",
        "from circuitsvis.activations import text_neuron_activations\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from itertools import product\n",
        "import openai\n",
        "from getpass import getpass\n",
        "import os\n",
        "import wandb\n",
        "from wandb import Artifact\n",
        "from wandb import Api\n",
        "from tqdm import tqdm\n",
        "import csv\n",
        "import nltk\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import zipfile\n",
        "import random\n",
        "import traceback"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "_1nJYB_oLiqi"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UB0OrAXz3sid"
      },
      "source": [
        "##Train Autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-OlhZEI05I50"
      },
      "outputs": [],
      "source": [
        "class SparseAutoencoder(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, l1_coef):\n",
        "        super(SparseAutoencoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = input_size\n",
        "\n",
        "        self.kwargs = {'input_size': input_size, 'hidden_size': hidden_size, 'l1_coef': l1_coef}\n",
        "        self.l1_coef = l1_coef\n",
        "\n",
        "        self.encoder_weight = nn.Parameter(torch.randn(hidden_size, input_size))\n",
        "        nn.init.orthogonal_(self.encoder_weight)\n",
        "\n",
        "        self.encoder_bias = nn.Parameter(torch.zeros(self.hidden_size))\n",
        "        self.decoder_bias = nn.Parameter(torch.zeros(input_size))\n",
        "\n",
        "    def forward(self, x):\n",
        "        normalized_encoder_weight = F.normalize(self.encoder_weight, p=2, dim=1)\n",
        "\n",
        "        features = F.linear(x, normalized_encoder_weight, self.encoder_bias)\n",
        "        features = F.relu(features)\n",
        "\n",
        "        reconstruction = F.linear(features, normalized_encoder_weight.t(), self.decoder_bias)\n",
        "\n",
        "        return features, reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "_NkYyYAogUXM",
        "outputId": "5ac44090-ec7e-49c1-ce9e-8c7f6e4f9332"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "wandb version 0.16.0 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.15.12"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20231122_013611-lvwu7vqi</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nlp_and_interpretability/Autoencoder_training_pythia_70m_utility_reward_interp/runs/lvwu7vqi' target=\"_blank\">breezy-monkey-13</a></strong> to <a href='https://wandb.ai/nlp_and_interpretability/Autoencoder_training_pythia_70m_utility_reward_interp' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/nlp_and_interpretability/Autoencoder_training_pythia_70m_utility_reward_interp' target=\"_blank\">https://wandb.ai/nlp_and_interpretability/Autoencoder_training_pythia_70m_utility_reward_interp</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/nlp_and_interpretability/Autoencoder_training_pythia_70m_utility_reward_interp/runs/lvwu7vqi' target=\"_blank\">https://wandb.ai/nlp_and_interpretability/Autoencoder_training_pythia_70m_utility_reward_interp/runs/lvwu7vqi</a>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "entity_name = 'nlp_and_interpretability'\n",
        "\n",
        "policy_model_name=\"pythia_70m_utility_reward\"\n",
        "project_prefix = 'Autoencoder_training'\n",
        "\n",
        "interp_project_name = f\"{project_prefix}_{policy_model_name}_interp\"\n",
        "run=wandb.init(project=interp_project_name)  # Add config = {dict_of_params_run} as keyword argument"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "aF5gjLJFMNJM"
      },
      "outputs": [],
      "source": [
        "entity_name = 'nlp_and_interpretability'\n",
        "project_prefix = 'Autoencoder_training'\n",
        "artifact_prefix = 'autoencoders'\n",
        "\n",
        "def save_models_to_folder(model_dict, save_dir):\n",
        "    \"\"\"\n",
        "    Save PyTorch models from a dictionary to a specified directory.\n",
        "\n",
        "    Args:\n",
        "        model_dict (dict): A dictionary containing PyTorch models with keys as model names.\n",
        "        save_dir (str): The directory where models will be saved.\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    for model_name, model_list in model_dict.items():\n",
        "        for i, model in enumerate(model_list):\n",
        "            model_path = os.path.join(save_dir, f'{model_name}')\n",
        "            torch.save([model.kwargs, model.state_dict()], model_path)\n",
        "            print(f\"Saved {model_name} to {model_path}\")\n",
        "\n",
        "def save_autoencoders_for_artifact(\n",
        "        autoencoders_base_big, autoencoders_base_small, autoencoders_rlhf_big, autoencoders_rlhf_small,\n",
        "        policy_model_name, hyperparameters, alias, run\n",
        "    ):\n",
        "    '''\n",
        "    Saves the autoencoders from one run into memory. Note that these paths are to some extent hardcoded\n",
        "    '''\n",
        "    save_dir = 'saves'\n",
        "    save_models_to_folder(autoencoders_base_big, save_dir=f'{save_dir}/base_big')\n",
        "    save_models_to_folder(autoencoders_base_small, save_dir=f'{save_dir}/base_small')\n",
        "    save_models_to_folder(autoencoders_rlhf_big, save_dir=f'{save_dir}/rlhf_big')\n",
        "    save_models_to_folder(autoencoders_rlhf_small, save_dir=f'{save_dir}/rlhf_small')\n",
        "\n",
        "    simplified_policy_name = policy_model_name.split('/')[-1].replace(\"-\", \"_\")\n",
        "    artifact_name = f'{artifact_prefix}_{simplified_policy_name}'\n",
        "    saved_artifact = Artifact(artifact_name, metadata=hyperparameters, type='model')\n",
        "    saved_artifact.add_dir(save_dir, name=save_dir)\n",
        "\n",
        "    aliases = {simplified_policy_name, 'latest', 'weights_tied'}\n",
        "    aliases.add(alias)\n",
        "    aliases = sorted(list(aliases))\n",
        "    run.log_artifact(saved_artifact, aliases=aliases)\n",
        "\n",
        "def load_autoencoders_for_artifact(policy_model_name, alias='latest', run=run):\n",
        "    '''\n",
        "    Loads the autoencoders from one run into memory. Note that these paths are to some extent hardcoded\n",
        "    For example, try autoencoders_dict = load_autoencoders_for_artifact('pythia_70m_sentiment_reward')\n",
        "    '''\n",
        "    simplified_policy_model_name = policy_model_name.split('/')[-1].replace('-', '_')\n",
        "    full_path = f'{entity_name}/{project_prefix}_{policy_model_name}/{artifact_prefix}_{simplified_policy_model_name}:{alias}'\n",
        "    print(f'Loading artifact from {full_path}')\n",
        "\n",
        "    artifact = run.use_artifact(full_path)\n",
        "    directory = artifact.download()\n",
        "\n",
        "    save_dir = f'{directory}/saves'\n",
        "    autoencoders_base_big = load_models_from_folder(f'{save_dir}/base_big')\n",
        "    autoencoders_base_small = load_models_from_folder(f'{save_dir}/base_small')\n",
        "    autoencoders_rlhf_big = load_models_from_folder(f'{save_dir}/rlhf_big')\n",
        "    autoencoders_rlhf_small = load_models_from_folder(f'{save_dir}/rlhf_small')\n",
        "\n",
        "    return {\n",
        "        'base_big': autoencoders_base_big, 'base_small': autoencoders_base_small,\n",
        "        'rlhf_big': autoencoders_rlhf_big, 'rlhf_small': autoencoders_rlhf_small\n",
        "    }\n",
        "\n",
        "def load_models_from_folder(load_dir):\n",
        "    \"\"\"\n",
        "    Load PyTorch models from subfolders of a directory into a dictionary where keys are subfolder names.\n",
        "\n",
        "    Args:\n",
        "        load_dir (str): The directory from which models will be loaded.\n",
        "\n",
        "    Returns:\n",
        "        model_dict (dict): A dictionary where keys are subfolder names and values are PyTorch models.\n",
        "    \"\"\"\n",
        "    model_dict = {}\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    for model_name in sorted(os.listdir(load_dir)):\n",
        "        model_path = os.path.join(load_dir, model_name)\n",
        "\n",
        "        kwargs, state = torch.load(model_path, map_location=device)\n",
        "\n",
        "        model = SparseAutoencoder(**kwargs)\n",
        "        model.load_state_dict(state)\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        model_dict[model_name] = model\n",
        "        print(f\"Loaded {model_name} from {model_path}\")\n",
        "\n",
        "    return model_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "XrXZr_T-Mp9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2075e1e-6293-4636-bf65-78ab59662333"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading artifact from nlp_and_interpretability/Autoencoder_training_pythia_70m_utility_reward/autoencoders_pythia_70m_utility_reward:latest\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m:   20 of 20 files downloaded.  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 1 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/base_big/1\n",
            "Loaded 2 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/base_big/2\n",
            "Loaded 3 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/base_big/3\n",
            "Loaded 4 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/base_big/4\n",
            "Loaded 5 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/base_big/5\n",
            "Loaded 1 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/base_small/1\n",
            "Loaded 2 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/base_small/2\n",
            "Loaded 3 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/base_small/3\n",
            "Loaded 4 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/base_small/4\n",
            "Loaded 5 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/base_small/5\n",
            "Loaded 1 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/rlhf_big/1\n",
            "Loaded 2 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/rlhf_big/2\n",
            "Loaded 3 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/rlhf_big/3\n",
            "Loaded 4 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/rlhf_big/4\n",
            "Loaded 5 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/rlhf_big/5\n",
            "Loaded 1 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/rlhf_small/1\n",
            "Loaded 2 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/rlhf_small/2\n",
            "Loaded 3 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/rlhf_small/3\n",
            "Loaded 4 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/rlhf_small/4\n",
            "Loaded 5 from ./artifacts/autoencoders_pythia_70m_utility_reward:v11/saves/rlhf_small/5\n"
          ]
        }
      ],
      "source": [
        "loaded_models_dict = load_autoencoders_for_artifact(policy_model_name=policy_model_name, alias=\"latest\", run=run)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YNPcqnbttq5u"
      },
      "source": [
        "# Quantify Reward Modeling Efficacy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzhGRlpcV-Fz"
      },
      "source": [
        "## Cosine Similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Rgrkog_oqWWU"
      },
      "outputs": [],
      "source": [
        "def calculate_MMCS_hungarian(small_weights, big_weights):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    small_weights = torch.tensor(small_weights).to(device)\n",
        "    big_weights = torch.tensor(big_weights).to(device)\n",
        "\n",
        "    small_weights_norm = torch.nn.functional.normalize(small_weights, p=2, dim=0)\n",
        "    big_weights_norm = torch.nn.functional.normalize(big_weights, p=2, dim=0)\n",
        "    cos_sims = torch.mm(small_weights_norm.T, big_weights_norm)\n",
        "    cos_sims_np = 1 - cos_sims.cpu().numpy()\n",
        "    row_ind, col_ind = linear_sum_assignment(cos_sims_np)\n",
        "    max_cosine_similarities = 1 - cos_sims_np[row_ind, col_ind]\n",
        "    mean_mmcs = np.mean(max_cosine_similarities)\n",
        "    sorted_indices = np.argsort(max_cosine_similarities)[::-1]\n",
        "\n",
        "    return mean_mmcs, sorted_indices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AH-kguI-8gO2"
      },
      "outputs": [],
      "source": [
        "def compare_autoencoders(small_dict, big_dict, top_k):\n",
        "    mmcs_results = {}\n",
        "\n",
        "    small_autoencoders_list = list(small_dict.values())\n",
        "    big_autoencoders_list = list(big_dict.values())\n",
        "    layer_names = list(small_dict.keys())\n",
        "\n",
        "    if len(small_autoencoders_list) != len(big_autoencoders_list):\n",
        "        raise ValueError(\"Length of small and big autoencoders lists must be the same.\")\n",
        "\n",
        "    for layer_name, (small_autoencoder, big_autoencoder) in zip(layer_names, zip(small_autoencoders_list, big_autoencoders_list)):\n",
        "        small_weights = small_autoencoder.encoder_weight.detach().cpu().numpy().T\n",
        "        big_weights = big_autoencoder.encoder_weight.detach().cpu().numpy().T\n",
        "\n",
        "        MMCS_value, sorted_indices = calculate_MMCS_hungarian(small_weights, big_weights)\n",
        "\n",
        "        top_k_indices = sorted_indices[:top_k].tolist()\n",
        "\n",
        "        mmcs_results[layer_name] = (MMCS_value, top_k_indices)\n",
        "\n",
        "    return mmcs_results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5VBcqhkIWDV4"
      },
      "source": [
        "## Tokenization and Activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "QQWjMXKzUfb_"
      },
      "outputs": [],
      "source": [
        "def add_space_prefix(tokens):\n",
        "    return ['Ġ' + token if not token.startswith('Ġ') else token for token in tokens]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "uiBq3takwD_f"
      },
      "outputs": [],
      "source": [
        "def tokenize_imdb_data(imdb_data, num_samples):\n",
        "    tokenized_data = []\n",
        "    tokenized_texts = []\n",
        "    for text in random.sample(imdb_data, num_samples):\n",
        "        dtokens = tokenizer(text[:50], return_tensors='pt', padding=True, truncation=True)\n",
        "        tokens = tokenizer.tokenize(text[:50])\n",
        "        tokens = add_space_prefix(tokens)\n",
        "        tokens_text = ' '.join(tokens)\n",
        "\n",
        "        tokenized_data.append(dtokens)\n",
        "        tokenized_texts.append(tokens_text)\n",
        "\n",
        "    return tokenized_data, tokenized_texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "xQe2UOYOHIBm"
      },
      "outputs": [],
      "source": [
        "def get_encoder_activations_and_reconstruction(input_ids_tensor, attention_mask_tensor, transformer_model, autoencoder_model):\n",
        "    with torch.no_grad():\n",
        "        activations = transformer_model(input_ids_tensor, attention_mask=attention_mask_tensor)[0]\n",
        "\n",
        "    activations = activations.float()\n",
        "    features, reconstruction = autoencoder_model(activations)\n",
        "\n",
        "    return features, reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Y05G41cTZJxi"
      },
      "outputs": [],
      "source": [
        "def normalize_activations(activations, max_activation):\n",
        "    activations[activations < 0] = 0\n",
        "    normalized_activations = 10 * activations / max_activation\n",
        "    return normalized_activations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "5fJwtX43VauW"
      },
      "outputs": [],
      "source": [
        "def discretize_activations(normalized_activations):\n",
        "    return np.round(normalized_activations).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tO_f0gc4VdPm"
      },
      "outputs": [],
      "source": [
        "def handle_sparse_activations(tokens, discretized_activations):\n",
        "    non_zero_indices = np.where(discretized_activations != 0)[0]\n",
        "    if len(non_zero_indices) / len(discretized_activations) < 0.2:\n",
        "        repeated_tokens = [tokens[i] for i in non_zero_indices]\n",
        "        repeated_activations = [discretized_activations[i] for i in non_zero_indices]\n",
        "        tokens += repeated_tokens\n",
        "        discretized_activations = np.concatenate([discretized_activations, repeated_activations])\n",
        "    return tokens, discretized_activations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTRXfB7PWJhX"
      },
      "source": [
        "## Autointerpretability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "KtFUbUJ7FZ5r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3281ab02-62d4-44f6-e868-f024b0d7e1df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API Key: ··········\n"
          ]
        }
      ],
      "source": [
        "key = getpass('API Key: ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "mtrMlz7UGGVr"
      },
      "outputs": [],
      "source": [
        "def get_feature_explanation(feature_index, top_5_activation_records_for_feature):\n",
        "    conversation = [\n",
        "        {\"role\": \"system\", \"content\": \"We're studying features in an autoencoder model. Each feature looks for some particular pattern in a short document. Look at the parts of the document the feature activates for and summarize in a single sentence what the feature is looking for.\"},\n",
        "        {\"role\": \"user\", \"content\": \"The activation format is token<tab>activation. Activation values range from 0 to 10. A feature finding what it's looking for is represented by a non-zero activation value. The higher the activation value, the stronger the match.\"},\n",
        "    ]\n",
        "\n",
        "    for record_idx, activation_str in enumerate(top_5_activation_records_for_feature):\n",
        "        user_message = f\"Feature {feature_index}\\nTop Activation Example {record_idx}:\\n{activation_str}\"\n",
        "        conversation.append({\"role\": \"user\", \"content\": user_message})\n",
        "\n",
        "    conversation.append({\"role\": \"user\", \"content\": f\"Explain what the feature at index {feature_index} in an autoencoder might be doing based on the top 5 activation records.\"})\n",
        "\n",
        "    api_key = key\n",
        "    model_engine = \"gpt-4\"\n",
        "\n",
        "    openai.api_key = api_key\n",
        "    response = openai.ChatCompletion.create(\n",
        "        model=model_engine,\n",
        "        messages=conversation\n",
        "    )\n",
        "\n",
        "    explanation = response['choices'][0]['message']['content'].strip()\n",
        "    return explanation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "N8grMYCbso10"
      },
      "outputs": [],
      "source": [
        "def get_activations_and_tensors(dtokens, model, autoencoder):\n",
        "    input_ids_tensor = dtokens['input_ids'].to(device)\n",
        "    attention_mask_tensor = dtokens.get('attention_mask', None)\n",
        "    if attention_mask_tensor is not None:\n",
        "        attention_mask_tensor = attention_mask_tensor.to(device)\n",
        "\n",
        "    features, real_activations = get_encoder_activations_and_reconstruction(input_ids_tensor, attention_mask_tensor, model, autoencoder)\n",
        "    return input_ids_tensor, attention_mask_tensor, features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9dsrUFn2WRrW"
      },
      "source": [
        "## Efficacy Scores and Correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "MWSFNGss1tgH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6979e1ee-baea-40ba-9394-20c5bdd4119c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "nltk.download('vader_lexicon')\n",
        "\n",
        "zip_file_path = '/root/nltk_data/sentiment/vader_lexicon.zip'\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/root/nltk_data/sentiment')\n",
        "\n",
        "lexicon_file_path = os.path.join('/root/nltk_data/sentiment', 'vader_lexicon/vader_lexicon.txt')\n",
        "\n",
        "vader_lexicon = {}\n",
        "with open(lexicon_file_path, 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for line in lines:\n",
        "        word, score = line.strip().split('\\t')[:2]\n",
        "        vader_lexicon[word] = float(score)\n",
        "\n",
        "def compute_sentiment(sentence):\n",
        "    words = sentence.lower().split()\n",
        "    if len(words) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    sentiment_score = 0.0\n",
        "    words_with_sentiment = 0\n",
        "    for word in words:\n",
        "        if word in vader_lexicon:\n",
        "            sentiment_score += vader_lexicon[word]\n",
        "            words_with_sentiment += 1\n",
        "\n",
        "    if words_with_sentiment == 0:\n",
        "        return 0.0\n",
        "\n",
        "    return sentiment_score / words_with_sentiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "Dazr-UK_tqTp"
      },
      "outputs": [],
      "source": [
        "def generate_completions(model, tokenizer, text_prefixes, max_length=100):\n",
        "    generator = pipeline('text-generation', model=model, tokenizer=tokenizer, device=\"cpu\")\n",
        "    completions_dict = {}\n",
        "    for prefix in text_prefixes:\n",
        "        generated = generator(prefix, max_length=max_length, num_return_sequences=1)\n",
        "        completion = generated[0]['generated_text'][len(prefix):].strip()\n",
        "        completions_dict[prefix] = completion\n",
        "    return completions_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "BBfWxaJUVsRu"
      },
      "outputs": [],
      "source": [
        "def calculate_utility_from_model(model, gen_model, tokenizer, autoencoders, imdb_data, num_samples, num_gen_samples, max_len, k, device=\"cpu\"):\n",
        "    tokenized_data, tokenized_texts  = tokenize_imdb_data(imdb_data, num_samples)\n",
        "    sample_texts = [text[:max_len] for text in imdb_data[:num_gen_samples]]\n",
        "\n",
        "    results_dict = {}\n",
        "    top_5_activation_records = defaultdict(dict)\n",
        "    similarity_results = compare_autoencoders(autoencoders['small'], autoencoders['big'], k + 15)\n",
        "    top_k_sum = 0\n",
        "    gen_utility = 0\n",
        "\n",
        "    for layer_name, (_, top_k_indices) in similarity_results.items():\n",
        "        autoencoder = autoencoders['big'][layer_name].to(device)\n",
        "        results_dict[layer_name] = {}\n",
        "\n",
        "        valid_features_processed = 0\n",
        "        for feature_index in top_k_indices:\n",
        "            if valid_features_processed >= k:\n",
        "                break\n",
        "\n",
        "            activations_for_feature = []\n",
        "\n",
        "            for dtokens in tokenized_data:\n",
        "                real_activations = get_activations_and_tensors(dtokens, model, autoencoder)[2][0, :, feature_index].detach().cpu().numpy()\n",
        "                max_activation = np.max(real_activations)\n",
        "                normalized_activations = normalize_activations(real_activations, max_activation)\n",
        "                discretized_activations = discretize_activations(normalized_activations)\n",
        "\n",
        "                activations_for_feature.append((dtokens, discretized_activations))\n",
        "\n",
        "            top_20_for_feature = sorted(activations_for_feature, key=lambda x: np.max(x[1]), reverse=True)[:20]\n",
        "\n",
        "            if len(top_20_for_feature) >= 5:\n",
        "                selected_activations = random.sample(top_20_for_feature, 5)\n",
        "            else:\n",
        "                selected_activations = top_20_for_feature\n",
        "\n",
        "            if all(np.max(activations) <= 0 for _, activations in selected_activations):\n",
        "                print(f\"Skipping feature index {feature_index} due to no significant activation\")\n",
        "                continue\n",
        "\n",
        "            valid_features_processed += 1\n",
        "            top_5_activation_records[layer_name][feature_index] = selected_activations\n",
        "\n",
        "            top_5_activation_examples = []\n",
        "            for dtokens, activations in selected_activations:\n",
        "                tokens = tokenizer.convert_ids_to_tokens(dtokens['input_ids'][0])\n",
        "                tokens, activations = handle_sparse_activations(tokens, activations)\n",
        "\n",
        "                activation_strings = [f\"{token}\\t{activation}\" for token, activation in zip(tokens, activations)]\n",
        "                top_5_activation_examples.append(\"\\n\".join(activation_strings))\n",
        "\n",
        "            results_dict[layer_name][feature_index] = get_feature_explanation(feature_index, top_5_activation_examples)\n",
        "\n",
        "    for layer_name in results_dict:\n",
        "\n",
        "        for neuron_index in results_dict[layer_name]:\n",
        "              explanation = results_dict[layer_name][neuron_index]\n",
        "              sentiment_score = compute_sentiment(explanation)\n",
        "              top_k_sum += abs(sentiment_score)\n",
        "\n",
        "    completions_dict = generate_completions(gen_model, tokenizer, sample_texts, max_length=50)\n",
        "\n",
        "    for prefix, completion in completions_dict.items():\n",
        "        sentiment_score = compute_sentiment(completion)\n",
        "        gen_utility += abs(sentiment_score)\n",
        "\n",
        "    print(results_dict)\n",
        "    return top_k_sum, gen_utility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_87CpTqWW78",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49016fc9-f6bc-4843-8d91-d31cf6fefdaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at amirabdullah19852020/pythia-70m_utility_reward were not used when initializing GPTNeoXForCausalLM: ['v_head.summary.weight', 'v_head.summary.bias']\n",
            "- This IS expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing GPTNeoXForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
            "<ipython-input-13-8f6527ee35a7>:3: RuntimeWarning: invalid value encountered in divide\n",
            "  normalized_activations = 10 * activations / max_activation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping feature index 510 due to no significant activation\n",
            "Skipping feature index 291 due to no significant activation\n",
            "Skipping feature index 351 due to no significant activation\n",
            "Skipping feature index 254 due to no significant activation\n",
            "Skipping feature index 378 due to no significant activation\n"
          ]
        }
      ],
      "source": [
        "#RLHF\n",
        "language_model = \"pythia-70m_utility_reward\"\n",
        "model = AutoModel.from_pretrained(f\"amirabdullah19852020/{language_model}\").to(device)\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(f\"amirabdullah19852020/{language_model}\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"amirabdullah19852020/{language_model}\")\n",
        "autoencoders = {'small': loaded_models_dict['rlhf_small'], 'big': loaded_models_dict['rlhf_big']}\n",
        "imdb_dataset = load_dataset('imdb', split='test')\n",
        "imdb_data = [entry['text'] for entry in imdb_dataset]\n",
        "sentiment_score = calculate_utility_from_model(model, gen_model, tokenizer, autoencoders, imdb_data, num_samples=500, num_gen_samples=100, max_len=30, k=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JAXp9_bHClwa"
      },
      "outputs": [],
      "source": [
        "print(sentiment_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmDQBW8TCO4k"
      },
      "outputs": [],
      "source": [
        "#BASE\n",
        "language_model = \"pythia-70m\"\n",
        "model = AutoModel.from_pretrained(f\"eleutherai/{language_model}\").to(device)\n",
        "gen_model = AutoModelForCausalLM.from_pretrained(f\"eleutherai/{language_model}\").to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(f\"eleutherai/{language_model}\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "autoencoders = {'small': loaded_models_dict['base_small'], 'big': loaded_models_dict['base_big']}\n",
        "imdb_dataset = load_dataset('imdb', split='test')\n",
        "imdb_data = [entry['text'] for entry in imdb_dataset]\n",
        "sentiment_score = calculate_utility_from_model(model, gen_model, tokenizer, autoencoders, imdb_data, num_samples=500, num_gen_samples=100, max_len=30, k=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U2XRaZTlCxUU"
      },
      "outputs": [],
      "source": [
        "print(sentiment_score)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}