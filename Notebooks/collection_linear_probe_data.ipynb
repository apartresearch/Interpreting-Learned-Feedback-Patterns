{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4545988f-e394-4a30-a80d-3d5c3540eaef",
   "metadata": {},
   "source": [
    "### Load training dataset and Vader sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a57fc8f-46ca-4ebf-a4e0-ecdadb9098c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "from time import time\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from torch import nn\n",
    "from tqdm import tqdm_notebook\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ddb89c9-9856-48b1-b346-a6f0bd8f2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-neo-125m'\n",
    "policy_model_name  = 'gpt_neo_125m_utility_reward'\n",
    "os.environ['WANDB_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8b6a48e-84cc-4db9-8501-dada3c0569a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamirali1985\u001b[0m (\u001b[33mnlp_and_interpretability\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b719880a55f74850bf562f5677acf89c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01111230937777792, max=1.0)â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/amir/work/codes/rlhf/Notebooks/wandb/run-20240129_072713-d4g9lxzu</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_and_interpretability/utility_reconstruction/runs/d4g9lxzu' target=\"_blank\">lemon-lake-60</a></strong> to <a href='https://wandb.ai/nlp_and_interpretability/utility_reconstruction' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_and_interpretability/utility_reconstruction' target=\"_blank\">https://wandb.ai/nlp_and_interpretability/utility_reconstruction</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_and_interpretability/utility_reconstruction/runs/d4g9lxzu' target=\"_blank\">https://wandb.ai/nlp_and_interpretability/utility_reconstruction/runs/d4g9lxzu</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init(project=\"utility_reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a0975b7-96d6-49ab-8df9-390378364143",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_name = f'EleutherAI/{model_name}'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(full_model_name)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(full_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afc9a76f-aa78-49f1-ae8a-23b3d715a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "lexicon = sentiment_analyzer.lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ba3ec-e7a8-4180-8ad4-e9c1280b5048",
   "metadata": {},
   "source": [
    "## Tokenization and torch utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b5533bf-faaf-47c0-9fd9-3d1c903586d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=1):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]\n",
    "\n",
    "def clear_gpu_memory():\n",
    "    start_time = time()\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    end_time = time()\n",
    "    total_time = round(end_time-start_time, 2)\n",
    "    print(f'Took {total_time} seconds to clear cache.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5dbbfcf-0577-4f2f-b987-7cc0ca71fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_lists(list_of_lists, pad_token):\n",
    "    max_length = max(len(lst) for lst in list_of_lists)\n",
    "    padded_list = [lst + [pad_token] * (max_length - len(lst)) for lst in list_of_lists]\n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95fb77c1-dfaa-437c-b344-7950146656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_number_of_tokens(word, tokenizer=tokenizer):\n",
    "    return len(tokenizer(word)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "61533633-3263-4edb-9c96-d73e5bc07c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_ids(text, tokenizer=tokenizer):\n",
    "    input_ids = tokenizer(text.lower(), truncation=True)['input_ids']\n",
    "    \n",
    "    tokens = [tokenizer.decode(input_id) for input_id in input_ids]\n",
    "    # The above produces artifacts such as a \" positive\" token and id, instead of \"positive\". So we redo this.\n",
    "\n",
    "    tokens = [token.lower().strip() for token in tokens]\n",
    "    tokenizer\n",
    "    return tokens, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e30a4f4c-22dd-4196-a7a8-b35bc9799bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_target_token_id(word, tokenizer=tokenizer):\n",
    "    word = word.lower().strip()\n",
    "    num_tokens = check_number_of_tokens(word)\n",
    "    if num_tokens > 1:\n",
    "        # Backoff to include a single space.\n",
    "        word = f' {word}'\n",
    "        num_tokens = check_number_of_tokens(word)\n",
    "\n",
    "    return tokenizer(word)['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e30cd968-56d3-41db-a0bd-598d9ea7b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextTokensIdsTarget:\n",
    "    attention_mask: list[int]\n",
    "    text: str\n",
    "    tokens: list[str]\n",
    "    ids: list[int]\n",
    "    target_token: str\n",
    "    target_token_id: int\n",
    "    target_token_position: int\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tensorized(datapoints: \"TextTokensIdsTarget\"):\n",
    "        max_length = max([len(datapoint.tokens) for datapoint in datapoints])\n",
    "        \n",
    "        input_ids = [datapoint.ids for datapoint in datapoints]\n",
    "        attention_masks = [datapoint.attention_mask for datapoint in datapoints]\n",
    "\n",
    "        input_ids_padded = pad_list_of_lists(input_ids, tokenizer.encode(tokenizer.pad_token)[0])\n",
    "        attention_masks_padded = pad_list_of_lists(attention_masks, 0)\n",
    "        all_tokenized = {\n",
    "            \"input_ids\": torch.IntTensor(input_ids_padded).cuda(), \"attention_mask\": torch.ByteTensor(attention_masks_padded).cuda()\n",
    "        }\n",
    "        return all_tokenized\n",
    "\n",
    "def trim_example(input_text: str, target_words: list[str], verbose=False, tokenizer=tokenizer):\n",
    "    single_target_token_ids = [get_single_target_token_id(word.strip().lower()) for word in target_words]\n",
    "    \n",
    "    single_target_token_ids = [token_id for token_id in single_target_token_ids if token_id]\n",
    "    single_target_tokens = [tokenizer.decode(token_id).strip().lower() for token_id in single_target_token_ids]\n",
    "\n",
    "    input_tokens, input_token_ids = get_tokens_and_ids(input_text)\n",
    "\n",
    "    trimmed_input_tokens = []\n",
    "    trimmed_input_token_ids = []\n",
    "\n",
    "    for input_token, input_token_id in zip(input_tokens, input_token_ids):\n",
    "        trimmed_input_tokens.append(input_token)\n",
    "        trimmed_input_token_ids.append(input_token_id)\n",
    "        if input_token.strip().lower() in single_target_tokens:\n",
    "            break\n",
    "\n",
    "    assert len(trimmed_input_token_ids) == len(trimmed_input_tokens), \"Num of tokens and token ids should be equal\"\n",
    "\n",
    "    last_token = None\n",
    "\n",
    "    if trimmed_input_tokens:\n",
    "        last_token = trimmed_input_tokens[-1].lower().strip()\n",
    "        last_token_id = trimmed_input_token_ids[-1]\n",
    "\n",
    "    if len(trimmed_input_tokens) > tokenizer.model_max_length:\n",
    "        print(f'Dropping example since exceed model max length. Input text was:\\n{input_text}')\n",
    "        return None\n",
    "    \n",
    "    elif last_token and last_token in single_target_tokens:\n",
    "        text = tokenizer.decode(trimmed_input_token_ids)\n",
    "        target_token_position = len(trimmed_input_token_ids) - 1\n",
    "        return TextTokensIdsTarget(\n",
    "            attention_mask=[1]*len(trimmed_input_tokens),\n",
    "            text=text, tokens=trimmed_input_tokens, ids=trimmed_input_token_ids, \n",
    "            target_token=last_token, target_token_id=last_token_id,\n",
    "            target_token_position=target_token_position\n",
    "        )\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f'last token was {last_token} in {trimmed_input_tokens}, and was not in target tokens.')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da5c75-e068-444c-9297-6a2bb7ef1924",
   "metadata": {},
   "source": [
    "### Load training examples for linear probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "913efa22-d8d2-4d1a-9f82-770e1052f59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "def load_wandb_json_artifact(\n",
    "    project_name='utility_reconstruction', artifact_name = 'contrastive_sentiment_pairs', version='v6'\n",
    "):\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(f'nlp_and_interpretability/{project_name}/{artifact_name}:{version}', type='data')\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "    with open(f'artifacts/{artifact_name}:{version}/{artifact_name}', 'r') as f_in:\n",
    "        result = json.load(f_in)\n",
    "        return result\n",
    "\n",
    "all_input_dicts = load_wandb_json_artifact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "90f7cfaa-6817-481a-a5b0-f6cad586dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_capitalization_neutral_terms(input_dicts):\n",
    "    for input_dict in input_dicts:\n",
    "        neutral_words = list(input_dict['neutral_words'].values())\n",
    "        for neutral_word in neutral_words:\n",
    "            input_dict['neutral_text'] = input_dict['neutral_text'].replace(neutral_word, neutral_word.lower())\n",
    "\n",
    "    return input_dicts\n",
    "\n",
    "all_input_dicts = clean_capitalization_neutral_terms(all_input_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2b517c39-b9d3-42c9-a121-b7310df0c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPoint:\n",
    "\n",
    "    def __init__(self, input_dict: dict, tokenizer=tokenizer):\n",
    "        self.input_dict = input_dict\n",
    "        self.positive_text = input_dict['input_text']\n",
    "        self.negative_text = input_dict['output_text']\n",
    "        self.neutral_text = input_dict['neutral_text']\n",
    "        \n",
    "        # Dictionary of layer name to activations by mlp layer.\n",
    "        self.activations: dict = None\n",
    "\n",
    "        # Dictionary of layer name to autoencoder feature by mlp layer\n",
    "        self.autoencoder_feature: dict = None\n",
    "\n",
    "        # Reward value of target_token.\n",
    "        self.target_positive_reward = None\n",
    "        self.target_negative_reward = None\n",
    "\n",
    "        self.positive_text_tokens, self.positive_input_ids = get_tokens_and_ids(self.positive_text)\n",
    "        self.negative_text_tokens, self.negative_token_ids = get_tokens_and_ids(self.negative_text)\n",
    "        \n",
    "        self.positive_words = input_dict['positive_words']\n",
    "        self.negative_words = list(input_dict['new_words'].values())\n",
    "        self.neutral_words = list(input_dict['neutral_words'].values())\n",
    "\n",
    "        self.target_positive_reward = None\n",
    "        self.target_positive_token = None\n",
    "        self.target_positive_token_id = None\n",
    "    \n",
    "        self.target_negative_reward = None\n",
    "        self.target_negative_token = None\n",
    "        self.target_negative_token_id = None\n",
    "\n",
    "        self.target_neutral_token = None\n",
    "        self.target_neutral_token_id = None\n",
    "\n",
    "        try:\n",
    "            self.trimmed_positive_example: TextTokensIdTarget = trim_example(self.positive_text, self.positive_words)\n",
    "            if self.trimmed_positive_example:\n",
    "                positive_token = self.trimmed_positive_example.target_token.strip().lower()\n",
    "                self.target_positive_reward = lexicon.get(positive_token, None)\n",
    "                self.target_positive_token = positive_token\n",
    "                self.target_positive_token_id = self.trimmed_positive_example.target_token_id\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict} for positive example.')\n",
    "            self.trimmed_positive_example = None\n",
    "        \n",
    "        try:\n",
    "            self.trimmed_negative_example: TextTokensIdTarget = trim_example(self.negative_text, self.negative_words)\n",
    "            if self.trimmed_negative_example:\n",
    "                negative_token = self.trimmed_negative_example.target_token.strip().lower()\n",
    "                self.target_negative_reward = lexicon.get(negative_token, None)\n",
    "                self.target_negative_token = negative_token\n",
    "                self.target_negative_token_id = self.trimmed_negative_example.target_token_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict} for negative example.')\n",
    "            self.trimmed_negative_example = None\n",
    "\n",
    "        try:\n",
    "            self.trimmed_neutral_example: TextTokensIdTarget = trim_example(self.neutral_text, self.neutral_words)\n",
    "            if self.trimmed_neutral_example:\n",
    "                self.target_neutral_token = self.trimmed_neutral_example.target_token.strip().lower()\n",
    "                self.target_neutral_token_id = self.trimmed_neutral_example.target_token_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict} for neutral example.')\n",
    "            self.trimmed_neutral_example = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65316eee-8b79-411d-abc4-e7ece073ea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6533/3914637853.py:2: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for input_dict in tqdm_notebook(all_input_dicts):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bcae9ef6ec4ede846f8da558051503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8640 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught exception list index out of range on {'input_text': 'It seems like anybody can make a movie nowadays.', 'output_text': 'It seems anybody can make a movie nowadays.', 'neutral_text': 'It seems tolerate anybody can make a movie nowadays.', 'positive_words': ['like'], 'new_words': {'like': ''}, 'neutral_words': {'like': 'tolerate'}} for negative example.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (4099 > 2048). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "training_points = []\n",
    "for input_dict in tqdm_notebook(all_input_dicts):\n",
    "    training_points.append(TrainingPoint(input_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8cc71f1e-d09d-415a-97f9-38a4fc911f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_training_points = [\n",
    "    training_point for training_point in training_points if \n",
    "    training_point.trimmed_positive_example and training_point.trimmed_negative_example\n",
    "    and training_point.trimmed_neutral_example\n",
    "    and training_point.target_positive_reward is not None\n",
    "    and training_point.target_negative_reward is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3297b711-b1f5-4a45-bde6-6fbb817247da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6601"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(successful_training_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b1f11bd-eaf4-4ee6-a5dd-eea748148853",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_points = successful_training_points[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52c1cf7f-b633-4bcf-bdc2-db6586ae1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_training_points = [training_point for training_point in  training_points if training_point not in successful_training_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c613185-b7fc-4f40-91b0-ea187fc94e46",
   "metadata": {},
   "source": [
    "### Load autoencoders for linear probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "52ff3e64-1584-4af4-9bbc-25dd78a0a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, l1_coef):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.kwargs = {'input_size': input_size, 'hidden_size': hidden_size, 'l1_coef': l1_coef}\n",
    "        self.l1_coef = l1_coef\n",
    "\n",
    "        self.encoder_weight = nn.Parameter(torch.randn(hidden_size, input_size))\n",
    "        nn.init.orthogonal_(self.encoder_weight)\n",
    "\n",
    "        self.encoder_bias = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_encoder_weight = F.normalize(self.encoder_weight, p=2, dim=1)\n",
    "\n",
    "        features = F.linear(x, normalized_encoder_weight, self.encoder_bias)\n",
    "        features = F.relu(features)\n",
    "\n",
    "        # reconstruction = F.linear(features, normalized_encoder_weight.t(), self.decoder_bias)\n",
    "        return features.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65622ee1-3281-45a6-bb29-0f80cc056a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_name = 'nlp_and_interpretability'\n",
    "project_prefix = 'Autoencoder_training'\n",
    "artifact_prefix = 'autoencoders'\n",
    "\n",
    "def load_autoencoders_for_artifact(policy_model_name, alias='latest', run=run):\n",
    "    '''\n",
    "    Loads the autoencoders from one run into memory. Note that these paths are to some extent hardcoded\n",
    "    For example, try autoencoders_dict = load_autoencoders_for_artifact('pythia_70m_sentiment_reward')\n",
    "    '''\n",
    "    simplified_policy_model_name = policy_model_name.split('/')[-1].replace('-', '_')\n",
    "    full_path = f'{entity_name}/{project_prefix}_{simplified_policy_model_name}/{artifact_prefix}_{simplified_policy_model_name}:{alias}'\n",
    "    print(f'Loading artifact from {full_path}')\n",
    "\n",
    "    artifact = run.use_artifact(full_path)\n",
    "    directory = artifact.download()\n",
    "\n",
    "    save_dir = f'{directory}/saves'\n",
    "    autoencoders_base_big = load_models_from_folder(load_dir=f'{save_dir}/base_big', given_device='cpu')\n",
    "    autoencoders_base_small = load_models_from_folder(load_dir=f'{save_dir}/base_small', given_device='cpu')\n",
    "    autoencoders_rlhf_big = load_models_from_folder(load_dir=f'{save_dir}/rlhf_big', given_device='cpu')\n",
    "    autoencoders_rlhf_small = load_models_from_folder(load_dir=f'{save_dir}/rlhf_small', given_device='cpu')\n",
    "\n",
    "    return {\n",
    "        'base_big': autoencoders_base_big, 'base_small': autoencoders_base_small,\n",
    "        'rlhf_big': autoencoders_rlhf_big, 'rlhf_small': autoencoders_rlhf_small\n",
    "    }\n",
    "\n",
    "def load_models_from_folder(load_dir, given_device=None):\n",
    "    \"\"\"\n",
    "    Load PyTorch models from subfolders of a directory into a dictionary where keys are subfolder names.\n",
    "\n",
    "    Args:\n",
    "        load_dir (str): The directory from which models will be loaded.\n",
    "\n",
    "    Returns:\n",
    "        model_dict (dict): A dictionary where keys are subfolder names and values are PyTorch models.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "\n",
    "    device = given_device if given_device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for model_name in sorted(os.listdir(load_dir)):\n",
    "        model_path = os.path.join(load_dir, model_name)\n",
    "\n",
    "        kwargs, state = torch.load(model_path, map_location=device)\n",
    "\n",
    "        model = SparseAutoencoder(**kwargs)\n",
    "        model.load_state_dict(state)\n",
    "        model.to(device)\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "\n",
    "        model_dict[model_name] = model\n",
    "        print(f\"Loaded {model_name} from {model_path}\")\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f914c2d1-5726-4a89-b98e-4e73cbe618fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading artifact from nlp_and_interpretability/Autoencoder_training_gpt_neo_125m_utility_reward/autoencoders_gpt_neo_125m_utility_reward:latest\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact autoencoders_gpt_neo_125m_utility_reward:latest, 67.67MB. 20 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   20 of 20 files downloaded.  \n",
      "Done. 0:0:0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 10 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/base_big/10\n",
      "Loaded 11 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/base_big/11\n",
      "Loaded 7 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/base_big/7\n",
      "Loaded 8 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/base_big/8\n",
      "Loaded 9 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/base_big/9\n",
      "Loaded 10 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/base_small/10\n",
      "Loaded 11 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/base_small/11\n",
      "Loaded 7 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/base_small/7\n",
      "Loaded 8 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/base_small/8\n",
      "Loaded 9 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/base_small/9\n",
      "Loaded 10 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/rlhf_big/10\n",
      "Loaded 11 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/rlhf_big/11\n",
      "Loaded 7 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/rlhf_big/7\n",
      "Loaded 8 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/rlhf_big/8\n",
      "Loaded 9 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/rlhf_big/9\n",
      "Loaded 10 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/rlhf_small/10\n",
      "Loaded 11 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/rlhf_small/11\n",
      "Loaded 7 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/rlhf_small/7\n",
      "Loaded 8 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/rlhf_small/8\n",
      "Loaded 9 from /data/home/amir/work/codes/rlhf/Notebooks/artifacts/autoencoders_gpt_neo_125m_utility_reward:v3/saves/rlhf_small/9\n"
     ]
    }
   ],
   "source": [
    "autoencoders_dictionaries = load_autoencoders_for_artifact(policy_model_name=policy_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "77e4d581-de44-4b8d-865c-63a95d93fe4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.h.0.mlp',\n",
       " 'transformer.h.1.mlp',\n",
       " 'transformer.h.2.mlp',\n",
       " 'transformer.h.3.mlp',\n",
       " 'transformer.h.4.mlp',\n",
       " 'transformer.h.5.mlp',\n",
       " 'transformer.h.6.mlp',\n",
       " 'transformer.h.7.mlp',\n",
       " 'transformer.h.8.mlp',\n",
       " 'transformer.h.9.mlp',\n",
       " 'transformer.h.10.mlp',\n",
       " 'transformer.h.11.mlp']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ModelCustomizer:\n",
    "    '''\n",
    "    Used to customize model layer numbers and other network parsing details\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize\n",
    "        '''\n",
    "        self.target_layers = None\n",
    "\n",
    "    def set_target_layers(self) -> list[str]:\n",
    "        '''\n",
    "        Set target layers\n",
    "        '''\n",
    "\n",
    "    def get_target_layers(self) -> list[str]:\n",
    "        '''\n",
    "        Get target layers.\n",
    "        '''\n",
    "\n",
    "    def parse_layer_name_to_layer_number(self, layer_name) -> str:\n",
    "        '''\n",
    "        Parse layer name to layer number\n",
    "        '''\n",
    "\n",
    "    def convert_ae_dict_keys(self, autoencoders_dict: [str, Tensor]):\n",
    "        '''\n",
    "        Parse ae dict keys to full layer names.\n",
    "        '''\n",
    "\n",
    "\n",
    "class GPTNeoCustomizer(ModelCustomizer):\n",
    "\n",
    "    def get_target_layers(self) -> list[str]:\n",
    "        if self.target_layers:\n",
    "            return self.target_layers\n",
    "        else:\n",
    "            return [self.layer_num_to_full_name(layer_no) for layer_no in range(12)]\n",
    "\n",
    "    def set_target_layers(self, target_layers):\n",
    "        self.target_layers = target_layers\n",
    "\n",
    "    def layer_num_to_full_name(self, layer_no):\n",
    "        return f'transformer.h.{layer_no}.mlp'\n",
    "\n",
    "    def parse_layer_name_to_layer_number(self, layer_name) -> str:\n",
    "        return layer_name.split('.')[-2]\n",
    "\n",
    "    # Standardize layer names to full names instead of 'int'\n",
    "    def convert_ae_dict_keys(self, autoencoders_dict: [str, Tensor]):\n",
    "        output_dict = {}\n",
    "        for key, autoencoder in autoencoders_dict.items():\n",
    "            output_dict[self.layer_num_to_full_name(key)] = autoencoder\n",
    "        return output_dict\n",
    "\n",
    "model_customizer = GPTNeoCustomizer()\n",
    "model_target_layers = model_customizer.get_target_layers()\n",
    "model_target_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747dae15-10ca-4a46-9c2d-b294baba4c0f",
   "metadata": {},
   "source": [
    "**Convert layer numbers to full layer names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8f8b072-77c2-4ff8-aac3-f4c8d5109e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dictionaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8993ba7e-fa07-450b-a7b6-234354b9568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, ae_dict in autoencoders_dictionaries.items():\n",
    "    mapped_dictionaries[key] = model_customizer.convert_ae_dict_keys(\n",
    "      ae_dict\n",
    "    )\n",
    "\n",
    "autoencoders_dictionaries = mapped_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e47c5055-16ac-4963-807a-517e0e0039aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.h.10.mlp',\n",
       " 'transformer.h.11.mlp',\n",
       " 'transformer.h.7.mlp',\n",
       " 'transformer.h.8.mlp',\n",
       " 'transformer.h.9.mlp']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rlhf_small = autoencoders_dictionaries['rlhf_small']\n",
    "rlhf_big = autoencoders_dictionaries['rlhf_big']\n",
    "\n",
    "model_customizer.set_target_layers(list(rlhf_small.keys()))\n",
    "model_customizer.get_target_layers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c54d67-22d7-4662-8752-9a6f20236e43",
   "metadata": {},
   "source": [
    "### Extract Activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a8a82ad-bc28-47c8-8bd5-3ff5657e5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsHook:\n",
    "    def __init__(self):\n",
    "        self.activations = []\n",
    "\n",
    "    def clear_activations(self):\n",
    "        for tensor in self.activations:\n",
    "            tensor = tensor.detach().cpu()\n",
    "        self.activations.clear()\n",
    "        self.activations = []\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        new_activations = torch.split(output.detach().cpu(), 1, dim=0)\n",
    "        self.activations.extend(new_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "285f2c36-5214-40a4-aba6-f6fb67199221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsExtractor:\n",
    "    def __init__(self, model, tokenizer, target_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Create an instance of ActivationHook\n",
    "        self.activation_hooks = {}\n",
    "        \n",
    "        for layer_name in self.target_layers:\n",
    "            activation_hook = ActivationsHook()\n",
    "            self.activation_hooks[layer_name] = activation_hook\n",
    "            layer = dict(model.named_modules())[layer_name]\n",
    "            # Register the forward hook to the chosen layer\n",
    "            hook_handle = layer.register_forward_hook(activation_hook.hook_fn)\n",
    "\n",
    "    def clear_all_activations(self):\n",
    "        for layer_name, activation_hook in self.activation_hooks.items():\n",
    "            activation_hook.clear_activations()\n",
    "\n",
    "    def get_activations(self):\n",
    "        \"\"\"\n",
    "        Retrieve all the cached activations.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            layer_name: activation_hook.activations for layer_name, activation_hook in self.activation_hooks.items()\n",
    "        }\n",
    "\n",
    "    def compute_activations_from_raw_texts(self, raw_texts: str):\n",
    "        self.clear_all_activations()\n",
    "\n",
    "        # Forward pass your input through the model\n",
    "        for text_batch in batch(texts):\n",
    "            input_data = self.tokenizer(text_batch, return_tensors='pt', padding=True)  # Example input shape\n",
    "            with torch.no_grad():\n",
    "                output = self.model(**input_data)\n",
    "\n",
    "        return self.get_activations()\n",
    "\n",
    "    def _flatten_activations(self, final_activations, num_samples):\n",
    "        flattened_activations = []\n",
    "        for i in range(num_samples):\n",
    "            current_activations = {}\n",
    "            for layer_name, activations_list in final_activations.items():\n",
    "                current_activations[layer_name] = [activations_list[i]]\n",
    "    \n",
    "            flattened_activations.append(current_activations)\n",
    "        \n",
    "        return flattened_activations\n",
    "\n",
    "    def compute_activations_from_text_tokens_ids_target(\n",
    "        self, samples: list[TextTokensIdsTarget], target_token_only=True, flatten=True\n",
    "    ):\n",
    "        self.clear_all_activations()\n",
    "\n",
    "        # Forward pass your input through the model\n",
    "        for text_batch in batch(samples):\n",
    "            tensorized = TextTokensIdsTarget.get_tensorized(text_batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = self.model(**tensorized)\n",
    "\n",
    "        all_activations = self.get_activations()\n",
    "        activations_per_layer = [len(value) for value in all_activations.values()]\n",
    "\n",
    "        assert max(activations_per_layer) == min(activations_per_layer) == len(samples), 'Each layer should have num_samples activations'\n",
    "\n",
    "        if target_token_only:\n",
    "            all_target_token_activations = {layer_num: [] for layer_num in all_activations}\n",
    "            for layer_num, layer_activations in all_activations.items():\n",
    "                assert len(layer_activations) == len(samples), \"Each layer should have same activations as num samples!\"\n",
    "\n",
    "                zipped_layer_activations_and_samples = zip(layer_activations, samples)\n",
    "                for activations, sample in zipped_layer_activations_and_samples:\n",
    "                    relevant_token_activations = activations[:, sample.target_token_position, :]\n",
    "                    all_target_token_activations[layer_num].append(relevant_token_activations)\n",
    "\n",
    "            final_activations = all_target_token_activations\n",
    "\n",
    "        else:\n",
    "            final_activations = all_activations\n",
    "\n",
    "        if flatten:\n",
    "            final_activations = self._flatten_activations(final_activations, num_samples=len(samples))\n",
    "            \n",
    "        else:\n",
    "            print(f'Returning a dictionary mapping layer name to list of activations')\n",
    "\n",
    "        return final_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d6f718e-b4f0-44b1-aa93-e526a7618e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ActivationsExtractor(model=model, target_layers=model_customizer.get_target_layers(), tokenizer=tokenizer)\n",
    "\n",
    "sample_positives = [point.trimmed_positive_example for point in sample_training_points]\n",
    "\n",
    "sample_target_token_activations = extractor.compute_activations_from_text_tokens_ids_target(sample_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4c6fed50-6ee6-4bc3-abce-08f91fed8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderManager:\n",
    "    def __init__(self, model, tokenizer, autoencoders_dict):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.autoencoders_dict = autoencoders_dict\n",
    "\n",
    "    def get_dictionary_features(self, activations, layer_name):\n",
    "        \"\"\"\n",
    "        Returns raw dictionary features for activations at a layer number.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.autoencoders_dict[layer_name](activations.cuda())\n",
    "            return features\n",
    "\n",
    "    def get_all_dictionary_features_for_list(self, activations_dict_list: list[dict[str, list[Tensor]]]):\n",
    "        return [self.get_all_dictionary_features_for_point(point) for point in activations_dict_list]\n",
    "        \n",
    "    def get_all_dictionary_features_for_point(self, activations_dict: dict[str, list[Tensor]]):\n",
    "        all_features = {}\n",
    "        for layer_name, autoencoder in self.autoencoders_dict.items():\n",
    "            activations = activations_dict[layer_name]\n",
    "            assert len(activations) == 1, \"Can only do conversion for single elements right now\"\n",
    "            curr_dict_features = self.get_dictionary_features(activations[0], layer_name)[0].tolist()\n",
    "            all_features[layer_name] = csr_matrix(curr_dict_features)\n",
    "        return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e6c9eb2-341b-40c6-a047-8efef12ea4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_manager = AutoencoderManager(\n",
    "    model=model, tokenizer=tokenizer, autoencoders_dict=rlhf_small\n",
    ")\n",
    "sample_features = autoencoder_manager.get_all_dictionary_features_for_list(\n",
    "    activations_dict_list = sample_target_token_activations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c004d5c9-8101-486d-8cb8-2215715e93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbeTrainingPoint:\n",
    "    def __init__(\n",
    "        self, training_point: TrainingPoint,\n",
    "        # positive token\n",
    "        target_positive_token_id: int,\n",
    "        target_positive_token: str,\n",
    "        positive_token_ae_features: [str, Tensor], \n",
    "        # negative token\n",
    "        target_negative_token_id: int,\n",
    "        target_negative_token: str,\n",
    "        negative_token_ae_features: [str, Tensor],\n",
    "        # neutral token\n",
    "        target_neutral_token_id: int,\n",
    "        target_neutral_token: str,\n",
    "        neutral_token_ae_features: [str, Tensor]\n",
    "    ):\n",
    "        self.training_point: TrainingPoint = training_point\n",
    "\n",
    "        self.target_positive_token = target_positive_token\n",
    "        self.target_positive_token_id = target_positive_token_id\n",
    "        self.target_positive_reward = self.training_point.target_positive_reward\n",
    "        self.positive_token_ae_features = positive_token_ae_features\n",
    "\n",
    "        self.target_negative_token = target_negative_token\n",
    "        self.target_negative_token_id = target_negative_token_id\n",
    "        self.target_negative_reward = self.training_point.target_negative_reward\n",
    "        self.negative_token_ae_features = negative_token_ae_features\n",
    "\n",
    "        self.target_neutral_token = target_neutral_token\n",
    "        self.target_neutral_token_id = target_neutral_token_id\n",
    "        self.neutral_token_ae_features = neutral_token_ae_features\n",
    "\n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "cd423a28-2889-4cae-8ef8-7959ea6da95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbeTrainingDataManager:\n",
    "    \"\"\"\n",
    "    This takes all the sample training data, and calculates all activations for all training samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, training_data: list[TrainingPoint], autoencoders_dict,\n",
    "        target_layers: list[str], model=model, tokenizer=tokenizer,\n",
    "    ):\n",
    "        self.activations_extractor = ActivationsExtractor(\n",
    "            model=model, tokenizer=tokenizer, target_layers=target_layers)\n",
    "        self.autoencoders_dict = autoencoders_dict\n",
    "        self.autoencoder_manager = AutoencoderManager(\n",
    "            model=model, tokenizer=tokenizer, autoencoders_dict=autoencoders_dict\n",
    "        )\n",
    "        self.training_data = training_data\n",
    "\n",
    "    def compute_training_points_single_batch(self, single_batch: list[TrainingPoint]):\n",
    "        positive_samples = [item.trimmed_positive_example for item in single_batch]\n",
    "        negative_samples = [item.trimmed_negative_example for item in single_batch]\n",
    "        neutral_samples = [item.trimmed_neutral_example for item in single_batch]\n",
    "    \n",
    "        positive_activations_list = self.activations_extractor.compute_activations_from_text_tokens_ids_target(\n",
    "            positive_samples, target_token_only=True, flatten=True\n",
    "        )\n",
    "        negative_activations_list = self.activations_extractor.compute_activations_from_text_tokens_ids_target(\n",
    "            negative_samples, target_token_only=True, flatten=True\n",
    "        )\n",
    "\n",
    "        neutral_activations_list = self.activations_extractor.compute_activations_from_text_tokens_ids_target(\n",
    "            neutral_samples, target_token_only=True, flatten=True\n",
    "        )\n",
    "\n",
    "        positive_dictionary_features = self.autoencoder_manager.get_all_dictionary_features_for_list(positive_activations_list)\n",
    "        negative_dictionary_features = self.autoencoder_manager.get_all_dictionary_features_for_list(negative_activations_list)\n",
    "        neutral_dictionary_features = self.autoencoder_manager.get_all_dictionary_features_for_list(neutral_activations_list)\n",
    "\n",
    "        assert (\n",
    "            len(positive_samples) == len(negative_samples) == len(neutral_samples) == \n",
    "            len(positive_activations_list) == len(negative_activations_list) ==\n",
    "            len(positive_dictionary_features) == len(negative_dictionary_features) == len(neutral_dictionary_features)\n",
    "        ), \"All samples, activations and dict features should align in length.\"\n",
    "\n",
    "        linear_probe_training_batch = []\n",
    "        zipped_point_and_features = zip(single_batch, positive_dictionary_features, negative_dictionary_features, neutral_dictionary_features)\n",
    "\n",
    "        for training_point, positive_features, negative_features, neutral_features in zipped_point_and_features:\n",
    "            linear_probe_training_point = LinearProbeTrainingPoint(\n",
    "                training_point=training_point,\n",
    "                positive_token_ae_features=positive_features, \n",
    "                negative_token_ae_features=negative_features,\n",
    "                neutral_token_ae_features=neutral_features,\n",
    "                # Positive token\n",
    "                target_positive_token_id=training_point.target_positive_token_id,\n",
    "                target_positive_token=training_point.target_positive_token,\n",
    "                # Negative token\n",
    "                target_negative_token_id=training_point.target_negative_token_id,\n",
    "                target_negative_token=training_point.target_negative_token,\n",
    "                # Neutral token\n",
    "                target_neutral_token_id=training_point.target_neutral_token_id,\n",
    "                target_neutral_token=training_point.target_neutral_token,\n",
    "            )\n",
    "            linear_probe_training_batch.append(linear_probe_training_point)\n",
    "\n",
    "        return linear_probe_training_batch\n",
    "\n",
    "    def construct_training_dataset(self, all_training_data: list[TrainingPoint], option: str = None):\n",
    "        \"\"\"\n",
    "        Constructs final training dataset, consisting of input and expected output. \n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        for index, single_batch in tqdm_notebook(enumerate(batch(all_training_data))):\n",
    "            if index % 250 == 0:\n",
    "                print(f'Clearing cuda cache on batch {index}')\n",
    "                clear_gpu_memory()\n",
    "    \n",
    "            current_results = self.compute_training_points_single_batch(single_batch)\n",
    "            all_results.extend(current_results)\n",
    "\n",
    "        assert len(all_results) == len(all_training_data), \"Not all training points were converted to probe inputs!\"\n",
    "\n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d897bdb9-3af1-4293-94c7-b4a9efede36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_data = successful_training_points[:5]\n",
    "\n",
    "lp_training_data_manager = LinearProbeTrainingDataManager(\n",
    "    training_data=sample_training_data, autoencoders_dict=rlhf_small,\n",
    "    model=model, tokenizer=tokenizer, target_layers = model_customizer.get_target_layers()    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5d047d5e-692e-4bcd-b645-a8dbff5db40b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6533/2706373509.py:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index, single_batch in tqdm_notebook(enumerate(batch(all_training_data))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21f15c2f2a814b8ea21440c31e0220a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing cuda cache on batch 0\n",
      "Took 0.28 seconds to clear cache.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' inspired great interest good good'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_probe_inputs = lp_training_data_manager.construct_training_dataset(sample_training_data)\n",
    "x =[input_point.target_positive_token_id for input_point in sample_probe_inputs]\n",
    "tokenizer.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f3b7330a-8273-4c0a-8e91-b388ac543eba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6533/2706373509.py:72: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  for index, single_batch in tqdm_notebook(enumerate(batch(all_training_data))):\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f26bace3162749efa9ee58b972fcd813",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clearing cuda cache on batch 0\n",
      "Took 0.27 seconds to clear cache.\n",
      "Clearing cuda cache on batch 250\n",
      "Took 0.27 seconds to clear cache.\n",
      "Clearing cuda cache on batch 500\n",
      "Took 0.27 seconds to clear cache.\n",
      "Clearing cuda cache on batch 750\n",
      "Took 0.28 seconds to clear cache.\n",
      "Clearing cuda cache on batch 1000\n",
      "Took 0.28 seconds to clear cache.\n",
      "Clearing cuda cache on batch 1250\n",
      "Took 0.29 seconds to clear cache.\n",
      "Clearing cuda cache on batch 1500\n",
      "Took 0.28 seconds to clear cache.\n",
      "Clearing cuda cache on batch 1750\n",
      "Took 0.3 seconds to clear cache.\n",
      "Clearing cuda cache on batch 2000\n",
      "Took 0.28 seconds to clear cache.\n",
      "Clearing cuda cache on batch 2250\n",
      "Took 0.29 seconds to clear cache.\n",
      "Clearing cuda cache on batch 2500\n",
      "Took 0.29 seconds to clear cache.\n",
      "Clearing cuda cache on batch 2750\n",
      "Took 0.3 seconds to clear cache.\n",
      "Clearing cuda cache on batch 3000\n",
      "Took 0.31 seconds to clear cache.\n",
      "Clearing cuda cache on batch 3250\n",
      "Took 0.31 seconds to clear cache.\n",
      "Clearing cuda cache on batch 3500\n",
      "Took 0.3 seconds to clear cache.\n",
      "Clearing cuda cache on batch 3750\n",
      "Took 0.33 seconds to clear cache.\n",
      "Clearing cuda cache on batch 4000\n",
      "Took 0.3 seconds to clear cache.\n",
      "Clearing cuda cache on batch 4250\n",
      "Took 0.31 seconds to clear cache.\n",
      "Clearing cuda cache on batch 4500\n",
      "Took 0.3 seconds to clear cache.\n",
      "Clearing cuda cache on batch 4750\n",
      "Took 0.34 seconds to clear cache.\n",
      "Clearing cuda cache on batch 5000\n",
      "Took 0.36 seconds to clear cache.\n",
      "Clearing cuda cache on batch 5250\n",
      "Took 0.38 seconds to clear cache.\n",
      "Clearing cuda cache on batch 5500\n",
      "Took 0.37 seconds to clear cache.\n",
      "Clearing cuda cache on batch 5750\n",
      "Took 0.39 seconds to clear cache.\n",
      "Clearing cuda cache on batch 6000\n",
      "Took 0.39 seconds to clear cache.\n",
      "Clearing cuda cache on batch 6250\n",
      "Took 0.33 seconds to clear cache.\n",
      "Clearing cuda cache on batch 6500\n",
      "Took 0.39 seconds to clear cache.\n"
     ]
    }
   ],
   "source": [
    "full_training_dataset = lp_training_data_manager.construct_training_dataset(successful_training_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f43506f8-f2c8-4294-ad1a-ff365bc0a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_dataset_to_wandb(training_dataset: list[LinearProbeTrainingPoint]):\n",
    "    out_filename = \"training_dataset.pkl\"\n",
    "\n",
    "    with open(out_filename, \"wb\") as f_out:\n",
    "        pickle.dump(training_dataset, f_out)\n",
    "    \n",
    "    my_artifact = wandb.Artifact(f\"linear_probe_training_dataset_{policy_model_name}\", type=\"data\")\n",
    "    \n",
    "    # Add the list to the artifact\n",
    "    my_artifact.add_file(local_path=out_filename, name=\"linear_probe_training_dataset\")\n",
    "\n",
    "    metadata_dict = {\n",
    "        \"description\": \"Training dataset, with activations and rewards\",\n",
    "        \"source\": \"Generated by my script\",\n",
    "        \"num_examples\": len(training_dataset),\n",
    "        \"split\": \"full\"\n",
    "    }\n",
    "\n",
    "    my_artifact.metadata.update(metadata_dict)\n",
    "\n",
    "    # Log the artifact to the run\n",
    "    wandb.log_artifact(my_artifact)\n",
    "\n",
    "save_training_dataset_to_wandb(full_training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d856f0-fe75-4f9d-928f-20afdcf5a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbeTrainer:\n",
    "    def __init__(self, linear_probe_training_data_manager):\n",
    "        \"\"\"\n",
    "        Initialize\n",
    "        \"\"\"\n",
    "    def train_linear_probe(self):\n",
    "        \"\"\"\n",
    "        Initialize\n",
    "        \"\"\"\n",
    "\n",
    "    def assign_reward(self, training_example):\n",
    "        \"\"\"\n",
    "        Assigns reward to aver\n",
    "        \"\"\"\n",
    "\n",
    "    def compute_divergence(self, training_example):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "    def average_reward(self, token):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "    def average_divergence(self, token):\n",
    "        \"\"\"\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
