{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4545988f-e394-4a30-a80d-3d5c3540eaef",
   "metadata": {},
   "source": [
    "### Load training dataset and Vader sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "5a57fc8f-46ca-4ebf-a4e0-ecdadb9098c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import json\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from torch import nn\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9ddb89c9-9856-48b1-b346-a6f0bd8f2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-neo-125m'\n",
    "os.environ['WANDB_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "id": "0a0975b7-96d6-49ab-8df9-390378364143",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_name = f'EleutherAI/{model_name}'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(full_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(full_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "afc9a76f-aa78-49f1-ae8a-23b3d715a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "lexicon = sentiment_analyzer.lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ba3ec-e7a8-4180-8ad4-e9c1280b5048",
   "metadata": {},
   "source": [
    "## Tokenization utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "95fb77c1-dfaa-437c-b344-7950146656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_number_of_tokens(word, tokenizer=tokenizer):\n",
    "    return len(tokenizer(word)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "id": "61533633-3263-4edb-9c96-d73e5bc07c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_ids(text, tokenizer=tokenizer):\n",
    "    input_ids = tokenizer(text.lower())['input_ids']\n",
    "    \n",
    "    tokens = [tokenizer.decode(input_id) for input_id in input_ids]\n",
    "    # The above produces artifacts such as a \" positive\" token and id, instead of \"positive\". So we redo this.\n",
    "\n",
    "    tokens = [token.lower().strip() for token in tokens]\n",
    "    tokenizer\n",
    "    return tokens, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "e30a4f4c-22dd-4196-a7a8-b35bc9799bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_target_token_id(word, tokenizer=tokenizer):\n",
    "    word = word.lower().strip()\n",
    "    num_tokens = check_number_of_tokens(word)\n",
    "    if num_tokens > 1:\n",
    "        # Backoff to include a single space.\n",
    "        word = f' {word}'\n",
    "        num_tokens = check_number_of_tokens(word)\n",
    "\n",
    "    return tokenizer(word)['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "e30cd968-56d3-41db-a0bd-598d9ea7b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextTokensIdsTarget:\n",
    "    text: str\n",
    "    tokens: list[str]\n",
    "    ids: list[int]\n",
    "    target_token: str\n",
    "    target_token_id: int\n",
    "\n",
    "def trim_example(input_text: str, target_words: list[str], verbose=False, tokenizer=tokenizer):\n",
    "    single_target_token_ids = [get_single_target_token_id(word.strip().lower()) for word in target_words]\n",
    "    \n",
    "    single_target_token_ids = [token_id for token_id in single_target_token_ids if token_id]\n",
    "    single_target_tokens = [tokenizer.decode(token_id).strip().lower() for token_id in single_target_token_ids]\n",
    "\n",
    "    input_tokens, input_token_ids = get_tokens_and_ids(input_text)\n",
    "\n",
    "    trimmed_input_tokens = []\n",
    "    trimmed_input_token_ids = []\n",
    "\n",
    "    for input_token, input_token_id in zip(input_tokens, input_token_ids):\n",
    "        trimmed_input_tokens.append(input_token)\n",
    "        trimmed_input_token_ids.append(input_token_id)\n",
    "        if input_token.strip().lower() in single_target_tokens:\n",
    "            break\n",
    "\n",
    "    last_token = None\n",
    "\n",
    "    if trimmed_input_tokens:\n",
    "        last_token = trimmed_input_tokens[-1].lower().strip()\n",
    "        last_token_id = trimmed_input_token_ids[-1]\n",
    "    \n",
    "    if last_token and last_token in single_target_tokens:\n",
    "        text = tokenizer.decode(trimmed_input_token_ids)\n",
    "        return TextTokensIdsTarget(\n",
    "            text=text, tokens=trimmed_input_tokens, ids=trimmed_input_token_ids, \n",
    "            target_token=last_token, target_token_id = last_token_id)\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f'last token was {last_token} in {trimmed_input_tokens}.')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da5c75-e068-444c-9297-6a2bb7ef1924",
   "metadata": {},
   "source": [
    "### Load training examples for linear probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "913efa22-d8d2-4d1a-9f82-770e1052f59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n"
     ]
    }
   ],
   "source": [
    "def load_wandb_json_artifact(\n",
    "    project_name='utility_reconstruction', artifact_name = 'contrastive_sentiment_pairs', version='v5'\n",
    "):\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(f'nlp_and_interpretability/{project_name}/{artifact_name}:{version}', type='data')\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "    with open(f'artifacts/{artifact_name}:{version}/{artifact_name}', 'r') as f_in:\n",
    "        result = json.load(f_in)\n",
    "        return result\n",
    "\n",
    "all_input_dicts = load_wandb_json_artifact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "id": "2b517c39-b9d3-42c9-a121-b7310df0c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPoint:\n",
    "\n",
    "    def __init__(self, input_dict: dict, tokenizer=tokenizer):\n",
    "        self.input_dict = input_dict\n",
    "        self.positive_text = input_dict['input_text']\n",
    "        self.negative_text = input_dict['output_text']\n",
    "\n",
    "        self.positive_text_tokens, self.positive_input_ids = get_tokens_and_ids(self.positive_text)\n",
    "        self.negative_text_tokens, self.negative_token_ids = get_tokens_and_ids(self.negative_text)\n",
    "        \n",
    "        self.positive_words = input_dict['positive_words']\n",
    "        self.negative_words = list(input_dict['new_words'].values())\n",
    "\n",
    "        try:\n",
    "            self.trimmed_positive_example: TextTokensIdTarget = trim_example(self.positive_text, self.positive_words)\n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict}.')\n",
    "            self.trimmed_positive_example = None\n",
    "        \n",
    "        try:\n",
    "            self.trimmed_negative_example: TextTokensIdTarget = trim_example(self.negative_text, self.negative_words)\n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict}.')\n",
    "            self.trimmed_negative_example = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "65316eee-8b79-411d-abc4-e7ece073ea7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Caught exception list index out of range on {'input_text': 'It seems like anybody can make a movie nowadays.', 'output_text': 'It seems anybody can make a movie nowadays.', 'positive_words': ['like'], 'new_words': {'like': ''}}.\n"
     ]
    }
   ],
   "source": [
    "training_points = [TrainingPoint(input_dict) for input_dict in all_input_dicts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "id": "8cc71f1e-d09d-415a-97f9-38a4fc911f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_training_points = [\n",
    "    training_point for training_point in training_points if \n",
    "    training_point.trimmed_positive_example and training_point.trimmed_negative_example\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "id": "3297b711-b1f5-4a45-bde6-6fbb817247da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8352"
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(successful_training_points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c613185-b7fc-4f40-91b0-ea187fc94e46",
   "metadata": {},
   "source": [
    "### Load autoencoders for linear probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c2c9149-4066-437c-95ae-e657d0ae440b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mamirali1985\u001b[0m (\u001b[33mnlp_and_interpretability\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/data/home/amir/work/codes/rlhf/Notebooks/wandb/run-20240123_142557-4rcut15u</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nlp_and_interpretability/rlhf-Notebooks/runs/4rcut15u' target=\"_blank\">glorious-wildflower-1</a></strong> to <a href='https://wandb.ai/nlp_and_interpretability/rlhf-Notebooks' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nlp_and_interpretability/rlhf-Notebooks' target=\"_blank\">https://wandb.ai/nlp_and_interpretability/rlhf-Notebooks</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nlp_and_interpretability/rlhf-Notebooks/runs/4rcut15u' target=\"_blank\">https://wandb.ai/nlp_and_interpretability/rlhf-Notebooks/runs/4rcut15u</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run = wandb.init('utility_reconstruction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "52ff3e64-1584-4af4-9bbc-25dd78a0a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, l1_coef):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.kwargs = {'input_size': input_size, 'hidden_size': hidden_size, 'l1_coef': l1_coef}\n",
    "        self.l1_coef = l1_coef\n",
    "\n",
    "        self.encoder_weight = nn.Parameter(torch.randn(hidden_size, input_size))\n",
    "        nn.init.orthogonal_(self.encoder_weight)\n",
    "\n",
    "        self.encoder_bias = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_encoder_weight = F.normalize(self.encoder_weight, p=2, dim=1)\n",
    "\n",
    "        features = F.linear(x, normalized_encoder_weight, self.encoder_bias)\n",
    "        features = F.relu(features)\n",
    "\n",
    "        reconstruction = F.linear(features, normalized_encoder_weight.t(), self.decoder_bias)\n",
    "\n",
    "        return features, reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "65622ee1-3281-45a6-bb29-0f80cc056a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_name = 'nlp_and_interpretability'\n",
    "project_prefix = 'Autoencoder_training'\n",
    "artifact_prefix = 'autoencoders'\n",
    "\n",
    "def load_autoencoders_for_artifact(policy_model_name, alias='latest', run=run):\n",
    "    '''\n",
    "    Loads the autoencoders from one run into memory. Note that these paths are to some extent hardcoded\n",
    "    For example, try autoencoders_dict = load_autoencoders_for_artifact('pythia_70m_sentiment_reward')\n",
    "    '''\n",
    "    simplified_policy_model_name = policy_model_name.split('/')[-1].replace('-', '_')\n",
    "    full_path = f'{entity_name}/{project_prefix}_{simplified_policy_model_name}/{artifact_prefix}_{simplified_policy_model_name}:{alias}'\n",
    "    print(f'Loading artifact from {full_path}')\n",
    "\n",
    "    artifact = run.use_artifact(full_path)\n",
    "    directory = artifact.download()\n",
    "\n",
    "    save_dir = f'{directory}/saves'\n",
    "    autoencoders_base_big = load_models_from_folder(f'{save_dir}/base_big')\n",
    "    autoencoders_base_small = load_models_from_folder(f'{save_dir}/base_small')\n",
    "    autoencoders_rlhf_big = load_models_from_folder(f'{save_dir}/rlhf_big')\n",
    "    autoencoders_rlhf_small = load_models_from_folder(f'{save_dir}/rlhf_small')\n",
    "\n",
    "    return {\n",
    "        'base_big': autoencoders_base_big, 'base_small': autoencoders_base_small,\n",
    "        'rlhf_big': autoencoders_rlhf_big, 'rlhf_small': autoencoders_rlhf_small\n",
    "    }\n",
    "\n",
    "def load_models_from_folder(load_dir):\n",
    "    \"\"\"\n",
    "    Load PyTorch models from subfolders of a directory into a dictionary where keys are subfolder names.\n",
    "\n",
    "    Args:\n",
    "        load_dir (str): The directory from which models will be loaded.\n",
    "\n",
    "    Returns:\n",
    "        model_dict (dict): A dictionary where keys are subfolder names and values are PyTorch models.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for model_name in sorted(os.listdir(load_dir)):\n",
    "        model_path = os.path.join(load_dir, model_name)\n",
    "\n",
    "        kwargs, state = torch.load(model_path, map_location=device)\n",
    "\n",
    "        model = SparseAutoencoder(**kwargs)\n",
    "        model.load_state_dict(state)\n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "\n",
    "        model_dict[model_name] = model\n",
    "        print(f\"Loaded {model_name} from {model_path}\")\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c54d67-22d7-4662-8752-9a6f20236e43",
   "metadata": {},
   "source": [
    "### Extract Activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "77e4d581-de44-4b8d-865c-63a95d93fe4f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['transformer.h.0.mlp',\n",
       " 'transformer.h.1.mlp',\n",
       " 'transformer.h.2.mlp',\n",
       " 'transformer.h.3.mlp',\n",
       " 'transformer.h.4.mlp',\n",
       " 'transformer.h.5.mlp',\n",
       " 'transformer.h.6.mlp',\n",
       " 'transformer.h.7.mlp',\n",
       " 'transformer.h.8.mlp',\n",
       " 'transformer.h.9.mlp',\n",
       " 'transformer.h.10.mlp',\n",
       " 'transformer.h.11.mlp']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_neo_target_layers = [f'transformer.h.{layer_no}.mlp' for layer_no in range(12)]\n",
    "gpt_neo_target_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "6b582ea5-cfc2-47fa-9c0d-d2dc4198b41a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_points = successful_training_points[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "36754901-bcb7-4c43-a4e9-cadfea63dc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationHook:\n",
    "    def __init__(self):\n",
    "        self.activations = []\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.activations.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "285f2c36-5214-40a4-aba6-f6fb67199221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsExtractor:\n",
    "    def __init__(self, model, target_layers, tokenizer):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Create an instance of ActivationHook\n",
    "        self.activation_hooks = {}\n",
    "        \n",
    "        for layer_name in self.target_layers:\n",
    "            activation_hook = ActivationHook()\n",
    "            self.activation_hooks[layer_name] = activation_hook\n",
    "            layer = dict(model.named_modules())[layer_name]\n",
    "            # Register the forward hook to the chosen layer\n",
    "            hook_handle = layer.register_forward_hook(activation_hook.hook_fn)\n",
    "\n",
    "    def get_activations(self, texts: list[str]):\n",
    "        # Forward pass your input through the model\n",
    "        input_data = self.tokenizer(texts, return_tensors='pt', padding=True)  # Example input shape\n",
    "        print(input_data)\n",
    "        output = self.model(**input_data)\n",
    "        print(f'We got {len(self.activation_hooks)}')\n",
    "\n",
    "activations_extractor = ActivationsExtractor(\n",
    "    model=model, target_layers=gpt_neo_target_layers, tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "0eaeb12f-1c5a-42c3-b97a-feefd1997827",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_texts = [text['input_text'] for text in all_texts]\n",
    "all_output_texts = [text['output_text'] for text in all_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "8a0d9b6f-266f-4f30-9d73-6e9159ad7891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 1212,   318,   257,  3967,  2478, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256],\n",
      "        [ 1212,   318,   257,  6507,  3074,   475,   356,   481,  3785,   428,\n",
      "           503,  1379,    13]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "We got 12\n"
     ]
    }
   ],
   "source": [
    "activations_extractor.get_activations([\"This is a positive development\", \"This is a sad situation but we will figure this out bro.\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "e2360f3f-848c-4b67-a62c-0c63779639d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 364,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(activations_extractor.activation_hooks['transformer.h.0.mlp'].activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "ed2f9cdb-3b45-44e3-a689-c40af7e8dad4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 13, 768])"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations_extractor.activation_hooks['transformer.h.1.mlp'].activations[].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "5a519a49-b97b-4de6-9472-8678946cb69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"This is a positive development\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(f'EleutherAI/{model_name}')\n",
    "input_ids = tokenizer.encode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9f98fdec-bcbd-4d3f-ad91-36ec7c11c1a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPTNeoConfig {\n",
       "  \"_name_or_path\": \"EleutherAI/gpt-neo-125m\",\n",
       "  \"activation_function\": \"gelu_new\",\n",
       "  \"architectures\": [\n",
       "    \"GPTNeoForCausalLM\"\n",
       "  ],\n",
       "  \"attention_dropout\": 0,\n",
       "  \"attention_layers\": [\n",
       "    \"global\",\n",
       "    \"local\",\n",
       "    \"global\",\n",
       "    \"local\",\n",
       "    \"global\",\n",
       "    \"local\",\n",
       "    \"global\",\n",
       "    \"local\",\n",
       "    \"global\",\n",
       "    \"local\",\n",
       "    \"global\",\n",
       "    \"local\"\n",
       "  ],\n",
       "  \"attention_types\": [\n",
       "    [\n",
       "      [\n",
       "        \"global\",\n",
       "        \"local\"\n",
       "      ],\n",
       "      6\n",
       "    ]\n",
       "  ],\n",
       "  \"bos_token_id\": 50256,\n",
       "  \"classifier_dropout\": 0.1,\n",
       "  \"embed_dropout\": 0,\n",
       "  \"eos_token_id\": 50256,\n",
       "  \"gradient_checkpointing\": false,\n",
       "  \"hidden_size\": 768,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": null,\n",
       "  \"layer_norm_epsilon\": 1e-05,\n",
       "  \"max_position_embeddings\": 2048,\n",
       "  \"model_type\": \"gpt_neo\",\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"resid_dropout\": 0,\n",
       "  \"summary_activation\": null,\n",
       "  \"summary_first_dropout\": 0.1,\n",
       "  \"summary_proj_to_labels\": true,\n",
       "  \"summary_type\": \"cls_index\",\n",
       "  \"summary_use_proj\": true,\n",
       "  \"transformers_version\": \"4.36.2\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 50257,\n",
       "  \"window_size\": 256\n",
       "}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.transformer.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823516ae-5215-4d46-ae5e-2e4b3f20fa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModifiedGPTNeoXForCausalLM(GPTNeoXForCausalLM):\n",
    "    def __init__(self, config, original_model=None):\n",
    "        super().__init__(config)\n",
    "        if original_model:\n",
    "            self.embed_in = original_model.embed_in\n",
    "            self.emb_dropout = original_model.emb_dropout\n",
    "            self.layers = original_model.layers\n",
    "            self.final_layer_norm = original_model.final_layer_norm\n",
    "\n",
    "    def custom_forward(self, reconstructed_activations, start_layer=1):\n",
    "        hidden_states = reconstructed_activations\n",
    "        for layer_module in self.layers[start_layer:]:\n",
    "            hidden_states = layer_module(hidden_states)[0]\n",
    "\n",
    "        if hasattr(self, 'final_layer_norm'):\n",
    "            hidden_states = self.final_layer_norm(hidden_states)\n",
    "\n",
    "        logits = self.embed_out(hidden_states)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34a07a24-cc42-48f7-af95-f067bb1a24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_reconstructed_activations(autoencoders_dict, layer_number, model, tokenizer, text):\n",
    "    with torch.no_grad():\n",
    "        embeddings = original_model.embed_in(input_ids_tensor)\n",
    "        embeddings = original_model.emb_dropout(embeddings)\n",
    "        first_layer_output = original_model.layers[0](embeddings)\n",
    "        mlp_activations = first_layer_output[0].float()\n",
    "\n",
    "        _, reconstructed_activations = autoencoder_model(mlp_activations)\n",
    "\n",
    "    return reconstructed_activations"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
