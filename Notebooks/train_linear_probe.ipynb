{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4545988f-e394-4a30-a80d-3d5c3540eaef",
   "metadata": {},
   "source": [
    "### Load training dataset and Vader sentiment analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a57fc8f-46ca-4ebf-a4e0-ecdadb9098c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "\n",
    "import nltk\n",
    "import spacy\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "import transformers\n",
    "import wandb\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from torch import nn\n",
    "from tqdm import tqdm_notebook\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddb89c9-9856-48b1-b346-a6f0bd8f2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt-neo-125m'\n",
    "policy_model_name  = 'gpt_neo_125m_utility_reward'\n",
    "os.environ['WANDB_API_KEY'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b6a48e-84cc-4db9-8501-dada3c0569a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=\"utility_reconstruction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0975b7-96d6-49ab-8df9-390378364143",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_model_name = f'EleutherAI/{model_name}'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(full_model_name)\n",
    "model.cuda()\n",
    "model.eval()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(full_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "#model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc9a76f-aa78-49f1-ae8a-23b3d715a5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "lexicon = sentiment_analyzer.lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd4ba3ec-e7a8-4180-8ad4-e9c1280b5048",
   "metadata": {},
   "source": [
    "## Tokenization utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5533bf-faaf-47c0-9fd9-3d1c903586d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch(iterable, n=16):\n",
    "    l = len(iterable)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable[ndx:min(ndx + n, l)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5dbbfcf-0577-4f2f-b987-7cc0ca71fafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_list_of_lists(list_of_lists, pad_token):\n",
    "    max_length = max(len(lst) for lst in list_of_lists)\n",
    "    padded_list = [lst + [pad_token] * (max_length - len(lst)) for lst in list_of_lists]\n",
    "    return padded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fb77c1-dfaa-437c-b344-7950146656d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_number_of_tokens(word, tokenizer=tokenizer):\n",
    "    return len(tokenizer(word)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61533633-3263-4edb-9c96-d73e5bc07c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens_and_ids(text, tokenizer=tokenizer):\n",
    "    input_ids = tokenizer(text.lower(), truncation=True)['input_ids']\n",
    "    \n",
    "    tokens = [tokenizer.decode(input_id) for input_id in input_ids]\n",
    "    # The above produces artifacts such as a \" positive\" token and id, instead of \"positive\". So we redo this.\n",
    "\n",
    "    tokens = [token.lower().strip() for token in tokens]\n",
    "    tokenizer\n",
    "    return tokens, input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30a4f4c-22dd-4196-a7a8-b35bc9799bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_single_target_token_id(word, tokenizer=tokenizer):\n",
    "    word = word.lower().strip()\n",
    "    num_tokens = check_number_of_tokens(word)\n",
    "    if num_tokens > 1:\n",
    "        # Backoff to include a single space.\n",
    "        word = f' {word}'\n",
    "        num_tokens = check_number_of_tokens(word)\n",
    "\n",
    "    return tokenizer(word)['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30cd968-56d3-41db-a0bd-598d9ea7b7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextTokensIdsTarget:\n",
    "    attention_mask: list[int]\n",
    "    text: str\n",
    "    tokens: list[str]\n",
    "    ids: list[int]\n",
    "    target_token: str\n",
    "    target_token_id: int\n",
    "    target_token_position: int\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tensorized(datapoints: \"TextTokensIdsTarget\"):\n",
    "        max_length = max([len(datapoint.tokens) for datapoint in datapoints])\n",
    "        \n",
    "        input_ids = [datapoint.ids for datapoint in datapoints]\n",
    "        attention_masks = [datapoint.attention_mask for datapoint in datapoints]\n",
    "\n",
    "        input_ids_padded = pad_list_of_lists(input_ids, tokenizer.encode(tokenizer.pad_token)[0])\n",
    "        attention_masks_padded = pad_list_of_lists(attention_masks, 0)\n",
    "        all_tokenized = {\n",
    "            \"input_ids\": torch.IntTensor(input_ids_padded).cuda(), \"attention_mask\": torch.ByteTensor(attention_masks_padded).cuda()\n",
    "        }\n",
    "        return all_tokenized\n",
    "\n",
    "def trim_example(input_text: str, target_words: list[str], verbose=False, tokenizer=tokenizer):\n",
    "    single_target_token_ids = [get_single_target_token_id(word.strip().lower()) for word in target_words]\n",
    "    \n",
    "    single_target_token_ids = [token_id for token_id in single_target_token_ids if token_id]\n",
    "    single_target_tokens = [tokenizer.decode(token_id).strip().lower() for token_id in single_target_token_ids]\n",
    "\n",
    "    input_tokens, input_token_ids = get_tokens_and_ids(input_text)\n",
    "\n",
    "    trimmed_input_tokens = []\n",
    "    trimmed_input_token_ids = []\n",
    "\n",
    "    for input_token, input_token_id in zip(input_tokens, input_token_ids):\n",
    "        trimmed_input_tokens.append(input_token)\n",
    "        trimmed_input_token_ids.append(input_token_id)\n",
    "        if input_token.strip().lower() in single_target_tokens:\n",
    "            break\n",
    "\n",
    "    assert len(trimmed_input_token_ids) == len(trimmed_input_tokens), \"Num of tokens and token ids should be equal\"\n",
    "\n",
    "    last_token = None\n",
    "\n",
    "    if trimmed_input_tokens:\n",
    "        last_token = trimmed_input_tokens[-1].lower().strip()\n",
    "        last_token_id = trimmed_input_token_ids[-1]\n",
    "\n",
    "    if len(trimmed_input_tokens) > tokenizer.model_max_length:\n",
    "        print(f'Dropping example since exceed model max length. Input text was:\\n{input_text}')\n",
    "        return None\n",
    "    \n",
    "    elif last_token and last_token in single_target_tokens:\n",
    "        text = tokenizer.decode(trimmed_input_token_ids)\n",
    "        target_token_position = len(trimmed_input_token_ids) - 1\n",
    "        return TextTokensIdsTarget(\n",
    "            attention_mask=[1]*len(trimmed_input_tokens),\n",
    "            text=text, tokens=trimmed_input_tokens, ids=trimmed_input_token_ids, \n",
    "            target_token=last_token, target_token_id=last_token_id,\n",
    "            target_token_position=target_token_position\n",
    "        )\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f'last token was {last_token} in {trimmed_input_tokens}, and was not in target tokens.')\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3da5c75-e068-444c-9297-6a2bb7ef1924",
   "metadata": {},
   "source": [
    "### Load training examples for linear probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913efa22-d8d2-4d1a-9f82-770e1052f59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wandb_json_artifact(\n",
    "    project_name='utility_reconstruction', artifact_name = 'contrastive_sentiment_pairs', version='v6'\n",
    "):\n",
    "    api = wandb.Api()\n",
    "    artifact = api.artifact(f'nlp_and_interpretability/{project_name}/{artifact_name}:{version}', type='data')\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "    with open(f'artifacts/{artifact_name}:{version}/{artifact_name}', 'r') as f_in:\n",
    "        result = json.load(f_in)\n",
    "        return result\n",
    "\n",
    "all_input_dicts = load_wandb_json_artifact()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f7cfaa-6817-481a-a5b0-f6cad586dcf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_capitalization_neutral_terms(input_dicts):\n",
    "    for input_dict in input_dicts:\n",
    "        neutral_words = list(input_dict['neutral_words'].values())\n",
    "        for neutral_word in neutral_words:\n",
    "            input_dict['neutral_text'] = input_dict['neutral_text'].replace(neutral_word, neutral_word.lower())\n",
    "\n",
    "    return input_dicts\n",
    "\n",
    "all_input_dicts = clean_capitalization_neutral_terms(all_input_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b517c39-b9d3-42c9-a121-b7310df0c3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainingPoint:\n",
    "\n",
    "    def __init__(self, input_dict: dict, tokenizer=tokenizer):\n",
    "        self.input_dict = input_dict\n",
    "        self.positive_text = input_dict['input_text']\n",
    "        self.negative_text = input_dict['output_text']\n",
    "        self.neutral_text = input_dict['neutral_text']\n",
    "        \n",
    "        # Dictionary of layer name to activations by mlp layer.\n",
    "        self.activations: dict = None\n",
    "\n",
    "        # Dictionary of layer name to autoencoder feature by mlp layer\n",
    "        self.autoencoder_feature: dict = None\n",
    "\n",
    "        # Reward value of target_token.\n",
    "        self.target_positive_reward = None\n",
    "        self.target_negative_reward = None\n",
    "\n",
    "        self.positive_text_tokens, self.positive_input_ids = get_tokens_and_ids(self.positive_text)\n",
    "        self.negative_text_tokens, self.negative_token_ids = get_tokens_and_ids(self.negative_text)\n",
    "        \n",
    "        self.positive_words = input_dict['positive_words']\n",
    "        self.negative_words = list(input_dict['new_words'].values())\n",
    "        self.neutral_words = list(input_dict['neutral_words'].values())\n",
    "\n",
    "        self.target_positive_reward = None\n",
    "        self.target_positive_token = None\n",
    "        self.target_positive_token_id = None\n",
    "    \n",
    "        self.target_negative_reward = None\n",
    "        self.target_negative_token = None\n",
    "        self.target_negative_token_id = None\n",
    "\n",
    "        self.target_neutral_token = None\n",
    "        self.target_neutral_token_id = None\n",
    "\n",
    "        try:\n",
    "            self.trimmed_positive_example: TextTokensIdTarget = trim_example(self.positive_text, self.positive_words)\n",
    "            if self.trimmed_positive_example:\n",
    "                positive_token = self.trimmed_positive_example.target_token.strip().lower()\n",
    "                self.target_positive_reward = lexicon.get(positive_token, None)\n",
    "                self.target_positive_token = positive_token\n",
    "                self.target_positive_token_id = self.trimmed_positive_example.target_token_id\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict} for positive example.')\n",
    "            self.trimmed_positive_example = None\n",
    "        \n",
    "        try:\n",
    "            self.trimmed_negative_example: TextTokensIdTarget = trim_example(self.negative_text, self.negative_words)\n",
    "            if self.trimmed_negative_example:\n",
    "                negative_token = self.trimmed_negative_example.target_token.strip().lower()\n",
    "                self.target_negative_reward = lexicon.get(negative_token, None)\n",
    "                self.target_negative_token = negative_token\n",
    "                self.target_negative_token_id = self.trimmed_negative_example.target_token_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict} for negative example.')\n",
    "            self.trimmed_negative_example = None\n",
    "\n",
    "        try:\n",
    "            self.trimmed_neutral_example: TextTokensIdTarget = trim_example(self.neutral_text, self.neutral_words)\n",
    "            if self.trimmed_neutral_example:\n",
    "                self.target_neutral_token = self.trimmed_neutral_example.target_token.strip().lower()\n",
    "                self.target_neutral_token_id = self.trimmed_neutral_example.target_token_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict} for neutral example.')\n",
    "            self.trimmed_neutral_example = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65316eee-8b79-411d-abc4-e7ece073ea7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_points = []\n",
    "for input_dict in tqdm_notebook(all_input_dicts):\n",
    "    training_points.append(TrainingPoint(input_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc71f1e-d09d-415a-97f9-38a4fc911f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "successful_training_points = [\n",
    "    training_point for training_point in training_points if \n",
    "    training_point.trimmed_positive_example and training_point.trimmed_negative_example\n",
    "    and training_point.target_positive_reward is not None\n",
    "    and training_point.target_negative_reward is not None\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3297b711-b1f5-4a45-bde6-6fbb817247da",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(successful_training_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1f11bd-eaf4-4ee6-a5dd-eea748148853",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_points = successful_training_points[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c1cf7f-b633-4bcf-bdc2-db6586ae1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_training_points = [training_point for training_point in  training_points if training_point not in successful_training_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c613185-b7fc-4f40-91b0-ea187fc94e46",
   "metadata": {},
   "source": [
    "### Load autoencoders for linear probe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ff3e64-1584-4af4-9bbc-25dd78a0a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, l1_coef):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.kwargs = {'input_size': input_size, 'hidden_size': hidden_size, 'l1_coef': l1_coef}\n",
    "        self.l1_coef = l1_coef\n",
    "\n",
    "        self.encoder_weight = nn.Parameter(torch.randn(hidden_size, input_size))\n",
    "        nn.init.orthogonal_(self.encoder_weight)\n",
    "\n",
    "        self.encoder_bias = nn.Parameter(torch.zeros(self.hidden_size))\n",
    "        self.decoder_bias = nn.Parameter(torch.zeros(input_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        normalized_encoder_weight = F.normalize(self.encoder_weight, p=2, dim=1)\n",
    "\n",
    "        features = F.linear(x, normalized_encoder_weight, self.encoder_bias)\n",
    "        features = F.relu(features)\n",
    "\n",
    "        reconstruction = F.linear(features, normalized_encoder_weight.t(), self.decoder_bias)\n",
    "\n",
    "        return features, reconstruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65622ee1-3281-45a6-bb29-0f80cc056a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_name = 'nlp_and_interpretability'\n",
    "project_prefix = 'Autoencoder_training'\n",
    "artifact_prefix = 'autoencoders'\n",
    "\n",
    "def load_autoencoders_for_artifact(policy_model_name, alias='latest', run=run):\n",
    "    '''\n",
    "    Loads the autoencoders from one run into memory. Note that these paths are to some extent hardcoded\n",
    "    For example, try autoencoders_dict = load_autoencoders_for_artifact('pythia_70m_sentiment_reward')\n",
    "    '''\n",
    "    simplified_policy_model_name = policy_model_name.split('/')[-1].replace('-', '_')\n",
    "    full_path = f'{entity_name}/{project_prefix}_{simplified_policy_model_name}/{artifact_prefix}_{simplified_policy_model_name}:{alias}'\n",
    "    print(f'Loading artifact from {full_path}')\n",
    "\n",
    "    artifact = run.use_artifact(full_path)\n",
    "    directory = artifact.download()\n",
    "\n",
    "    save_dir = f'{directory}/saves'\n",
    "    autoencoders_base_big = load_models_from_folder(load_dir=f'{save_dir}/base_big', given_device='cpu')\n",
    "    autoencoders_base_small = load_models_from_folder(load_dir=f'{save_dir}/base_small', given_device='cpu')\n",
    "    autoencoders_rlhf_big = load_models_from_folder(load_dir=f'{save_dir}/rlhf_big', given_device='cpu')\n",
    "    autoencoders_rlhf_small = load_models_from_folder(load_dir=f'{save_dir}/rlhf_small', given_device='cpu')\n",
    "\n",
    "    return {\n",
    "        'base_big': autoencoders_base_big, 'base_small': autoencoders_base_small,\n",
    "        'rlhf_big': autoencoders_rlhf_big, 'rlhf_small': autoencoders_rlhf_small\n",
    "    }\n",
    "\n",
    "def load_models_from_folder(load_dir, given_device=None):\n",
    "    \"\"\"\n",
    "    Load PyTorch models from subfolders of a directory into a dictionary where keys are subfolder names.\n",
    "\n",
    "    Args:\n",
    "        load_dir (str): The directory from which models will be loaded.\n",
    "\n",
    "    Returns:\n",
    "        model_dict (dict): A dictionary where keys are subfolder names and values are PyTorch models.\n",
    "    \"\"\"\n",
    "    model_dict = {}\n",
    "\n",
    "    device = given_device if given_device else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    for model_name in sorted(os.listdir(load_dir)):\n",
    "        model_path = os.path.join(load_dir, model_name)\n",
    "\n",
    "        kwargs, state = torch.load(model_path, map_location=device)\n",
    "\n",
    "        model = SparseAutoencoder(**kwargs)\n",
    "        model.load_state_dict(state)\n",
    "        model.to(device)\n",
    "        model.cuda()\n",
    "        model.eval()\n",
    "\n",
    "        model_dict[model_name] = model\n",
    "        print(f\"Loaded {model_name} from {model_path}\")\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f914c2d1-5726-4a89-b98e-4e73cbe618fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoders_dictionaries = load_autoencoders_for_artifact(policy_model_name=policy_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e4d581-de44-4b8d-865c-63a95d93fe4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelCustomizer:\n",
    "    '''\n",
    "    Used to customize model layer numbers and other network parsing details\n",
    "    '''\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Initialize\n",
    "        '''\n",
    "\n",
    "    def get_target_layers(self) -> list[str]:\n",
    "        '''\n",
    "        Get target layers.\n",
    "        '''\n",
    "\n",
    "    def parse_layer_name_to_layer_number(self, layer_name) -> str:\n",
    "        '''\n",
    "        Parse layer name to layer number\n",
    "        '''\n",
    "\n",
    "    def convert_ae_dict_keys(self, autoencoders_dict: [str, Tensor]):\n",
    "        '''\n",
    "        Parse ae dict keys to full layer names.\n",
    "        '''\n",
    "\n",
    "\n",
    "class GPTNeoCustomizer(ModelCustomizer):\n",
    "\n",
    "    def get_target_layers(self) -> list[str]:\n",
    "        return [self.layer_num_to_full_name(layer_no) for layer_no in range(12)]\n",
    "\n",
    "    def layer_num_to_full_name(self, layer_no):\n",
    "        return f'transformer.h.{layer_no}.mlp'\n",
    "\n",
    "    def parse_layer_name_to_layer_number(self, layer_name) -> str:\n",
    "        return layer_name.split('.')[-2]\n",
    "\n",
    "    # Standardize layer names to full names instead of 'int'\n",
    "    def convert_ae_dict_keys(self, autoencoders_dict: [str, Tensor]):\n",
    "        output_dict = {}\n",
    "        for key, autoencoder in autoencoders_dict.items():\n",
    "            output_dict[self.layer_num_to_full_name(key)] = autoencoder\n",
    "        return output_dict\n",
    "\n",
    "model_customizer = GPTNeoCustomizer()\n",
    "model_target_layers = model_customizer.get_target_layers()\n",
    "model_target_layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747dae15-10ca-4a46-9c2d-b294baba4c0f",
   "metadata": {},
   "source": [
    "**Convert layer numbers to full layer names.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f8b072-77c2-4ff8-aac3-f4c8d5109e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_dictionaries = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993ba7e-fa07-450b-a7b6-234354b9568e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, ae_dict in autoencoders_dictionaries.items():\n",
    "    mapped_dictionaries[key] = model_customizer.convert_ae_dict_keys(\n",
    "      ae_dict\n",
    "    )\n",
    "\n",
    "autoencoders_dictionaries = mapped_dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c5055-16ac-4963-807a-517e0e0039aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rlhf_small = autoencoders_dictionaries['rlhf_small']\n",
    "rlhf_big = autoencoders_dictionaries['rlhf_big']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c54d67-22d7-4662-8752-9a6f20236e43",
   "metadata": {},
   "source": [
    "### Extract Activations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8a82ad-bc28-47c8-8bd5-3ff5657e5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsHook:\n",
    "    def __init__(self):\n",
    "        self.activations = []\n",
    "\n",
    "    def clear_activations(self):\n",
    "        del self.activations\n",
    "        self.activations = []\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        new_activations = torch.split(output, 1, dim=0)\n",
    "        self.activations.extend(new_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285f2c36-5214-40a4-aba6-f6fb67199221",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActivationsExtractor:\n",
    "    def __init__(self, model, tokenizer, target_layers):\n",
    "        self.model = model\n",
    "        self.target_layers = target_layers\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # Create an instance of ActivationHook\n",
    "        self.activation_hooks = {}\n",
    "        \n",
    "        for layer_name in self.target_layers:\n",
    "            activation_hook = ActivationsHook()\n",
    "            self.activation_hooks[layer_name] = activation_hook\n",
    "            layer = dict(model.named_modules())[layer_name]\n",
    "            # Register the forward hook to the chosen layer\n",
    "            hook_handle = layer.register_forward_hook(activation_hook.hook_fn)\n",
    "\n",
    "    def clear_all_activations(self):\n",
    "        for layer_name, activation_hook in self.activation_hooks.items():\n",
    "            activation_hook.clear_activations()\n",
    "\n",
    "    def get_activations(self):\n",
    "        \"\"\"\n",
    "        Retrieve all the cached activations.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            layer_name: activation_hook.activations for layer_name, activation_hook in self.activation_hooks.items()\n",
    "        }\n",
    "\n",
    "    def compute_activations_from_raw_texts(self, raw_texts: str):\n",
    "        self.clear_all_activations()\n",
    "\n",
    "        # Forward pass your input through the model\n",
    "        for text_batch in batch(texts):\n",
    "            input_data = self.tokenizer(text_batch, return_tensors='pt', padding=True)  # Example input shape\n",
    "            with torch.no_grad():\n",
    "                output = self.model(**input_data)\n",
    "\n",
    "        return self.get_activations()\n",
    "\n",
    "    def _flatten_activations(self, final_activations, num_samples):\n",
    "        flattened_activations = []\n",
    "        for i in range(num_samples):\n",
    "            current_activations = {}\n",
    "            for layer_name, activations_list in final_activations.items():\n",
    "                current_activations[layer_name] = [activations_list[i]]\n",
    "    \n",
    "            flattened_activations.append(current_activations)\n",
    "        \n",
    "        return flattened_activations\n",
    "\n",
    "    def compute_activations_from_text_tokens_ids_target(\n",
    "        self, samples: list[TextTokensIdsTarget], target_token_only=True, flatten=True\n",
    "    ):\n",
    "        self.clear_all_activations()\n",
    "\n",
    "        # Forward pass your input through the model\n",
    "        for text_batch in batch(samples):\n",
    "            tensorized = TextTokensIdsTarget.get_tensorized(text_batch)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = self.model(**tensorized)\n",
    "\n",
    "        all_activations = self.get_activations()\n",
    "        activations_per_layer = [len(value) for value in all_activations.values()]\n",
    "\n",
    "        assert max(activations_per_layer) == min(activations_per_layer) == len(samples), 'Each layer should have num_samples activations'\n",
    "\n",
    "        if target_token_only:\n",
    "            all_target_token_activations = {layer_num: [] for layer_num in all_activations}\n",
    "            for layer_num, layer_activations in all_activations.items():\n",
    "                assert len(layer_activations) == len(samples), \"Each layer should have same activations as num samples!\"\n",
    "\n",
    "                zipped_layer_activations_and_samples = zip(layer_activations, samples)\n",
    "                for activations, sample in zipped_layer_activations_and_samples:\n",
    "                    relevant_token_activations = activations[:, sample.target_token_position, :]\n",
    "                    all_target_token_activations[layer_num].append(relevant_token_activations)\n",
    "                    \n",
    "            final_activations = all_target_token_activations\n",
    "\n",
    "        else:\n",
    "            final_activations = all_activations\n",
    "\n",
    "        if flatten:\n",
    "            final_activations = self._flatten_activations(final_activations, num_samples=len(samples))\n",
    "            \n",
    "        else:\n",
    "            print(f'Returning a dictionary mapping layer name to list of activations')\n",
    "\n",
    "        return final_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6f718e-b4f0-44b1-aa93-e526a7618e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = ActivationsExtractor(model=model, target_layers=model_customizer.get_target_layers(), tokenizer=tokenizer)\n",
    "\n",
    "sample_positives = [point.trimmed_positive_example for point in sample_training_points]\n",
    "\n",
    "sample_target_token_activations = extractor.compute_activations_from_text_tokens_ids_target(4*sample_positives)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf8ecbde-8bb4-4d96-acd4-1ad2de406d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_target_token_activations[4]\n",
    "len(sample_target_token_activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c6fed50-6ee6-4bc3-abce-08f91fed8166",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AutoencoderManager:\n",
    "    def __init__(self, model, tokenizer, autoencoders_dict):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.autoencoders_dict = autoencoders_dict\n",
    "\n",
    "    def get_dictionary_features(self, activations, layer_name):\n",
    "        \"\"\"\n",
    "        Returns raw dictionary features for activations at a layer number.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            features, reconstruction = self.autoencoders_dict[layer_name](activations.cuda())\n",
    "            return features\n",
    "\n",
    "    def get_all_dictionary_features_for_list(self, activations_dict_list: list[dict[str, list[Tensor]]]):\n",
    "        return [self.get_all_dictionary_features_for_point(point) for point in activations_dict_list]\n",
    "        \n",
    "    def get_all_dictionary_features_for_point(self, activations_dict: dict[str, list[Tensor]]):\n",
    "        all_features = {}\n",
    "        for layer_name, autoencoder in self.autoencoders_dict.items():\n",
    "            activations = activations_dict[layer_name]\n",
    "            assert len(activations) == 1, \"Can only do conversion for single elements right now\"\n",
    "            curr_dict_features = self.get_dictionary_features(activations[0], layer_name)[0].tolist()\n",
    "            all_features[layer_name] = csr_matrix(curr_dict_features)\n",
    "        return all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6c9eb2-341b-40c6-a047-8efef12ea4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder_manager = AutoencoderManager(\n",
    "    model=model, tokenizer=tokenizer, autoencoders_dict=rlhf_small\n",
    ")\n",
    "sample_features = autoencoder_manager.get_all_dictionary_features_for_list(\n",
    "    activations_dict_list = sample_target_token_activations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c004d5c9-8101-486d-8cb8-2215715e93f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbeTrainingPoint:\n",
    "    def __init__(\n",
    "        self, training_point: TrainingPoint,\n",
    "        target_positive_token_id: int,\n",
    "        target_negative_token_id: int,\n",
    "        target_positive_token: str,\n",
    "        target_negative_token: str,\n",
    "        positive_token_ae_features: [str, Tensor], \n",
    "        negative_token_ae_features: [str, Tensor],\n",
    "        neutral_token_ae_features: [str, Tensor] = None\n",
    "    ):\n",
    "        self.training_point: TrainingPoint = training_point\n",
    "\n",
    "        self.target_positive_token_id = target_positive_token_id\n",
    "        self.target_negative_token_id = target_negative_token_id\n",
    "        \n",
    "        self.target_positive_reward = self.training_point.target_positive_reward\n",
    "        self.target_negative_reward = self.training_point.target_negative_reward\n",
    "\n",
    "        self.positive_token_ae_features = positive_token_ae_features\n",
    "        self.negative_token_ae_features = negative_token_ae_features\n",
    "        self.neutral_token_ae_features = neutral_token_ae_features\n",
    "\n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd423a28-2889-4cae-8ef8-7959ea6da95a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbeTrainingDataManager:\n",
    "    \"\"\"\n",
    "    This takes all the sample training data, and calculates all activations for all training samples\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, training_data: list[TrainingPoint], autoencoders_dict,\n",
    "        target_layers: list[str], model=model, tokenizer=tokenizer,\n",
    "    ):\n",
    "        self.activations_extractor = ActivationsExtractor(\n",
    "            model=model, tokenizer=tokenizer, target_layers=target_layers)\n",
    "        self.autoencoders_dict = autoencoders_dict\n",
    "        self.autoencoder_manager = AutoencoderManager(\n",
    "            model=model, tokenizer=tokenizer, autoencoders_dict=autoencoders_dict)\n",
    "        self.training_data = training_data\n",
    "\n",
    "    def compute_training_points_single_batch(self, single_batch: list[TrainingPoint]):\n",
    "        positive_samples = [item.trimmed_positive_example for item in single_batch]\n",
    "        negative_samples = [item.trimmed_negative_example for item in single_batch]\n",
    "        neutral_samples = [item.trimmed_neutral_example for item in single_batch]\n",
    "    \n",
    "        positive_activations_list = self.activations_extractor.compute_activations_from_text_tokens_ids_target(\n",
    "            positive_samples, target_token_only=True, flatten=True\n",
    "        )\n",
    "        negative_activations_list = self.activations_extractor.compute_activations_from_text_tokens_ids_target(\n",
    "            negative_samples, target_token_only=True, flatten=True\n",
    "        )\n",
    "\n",
    "        positive_dictionary_features = self.autoencoder_manager.get_all_dictionary_features_for_list(positive_activations_list)\n",
    "        negative_dictionary_features = self.autoencoder_manager.get_all_dictionary_features_for_list(negative_activations_list)\n",
    "\n",
    "        assert (\n",
    "            len(positive_samples) == len(negative_samples) == len(neutral_samples) == \n",
    "            len(positive_activations_list) == len(negative_activations_list) ==\n",
    "            len(positive_dictionary_features) == len(negative_dictionary_features)\n",
    "        ), \"All samples, activations and dict features should align in length.\"\n",
    "\n",
    "        linear_probe_training_batch = []\n",
    "        zipped_point_and_features = zip(single_batch, positive_dictionary_features, negative_dictionary_features)\n",
    "\n",
    "        for training_point, positive_features, negative_features in zipped_point_and_features:\n",
    "            linear_probe_training_point = LinearProbeTrainingPoint(\n",
    "                training_point=training_point,\n",
    "                positive_token_ae_features=positive_features, \n",
    "                negative_token_ae_features=negative_features,\n",
    "                neutral_token_ae_features=None,\n",
    "                target_positive_token_id=training_point.target_positive_token_id,\n",
    "                target_negative_token_id=training_point.target_negative_token_id,\n",
    "                target_positive_token=training_point.target_positive_token,\n",
    "                target_negative_token=training_point.target_negative_token\n",
    "            )\n",
    "            linear_probe_training_batch.append(linear_probe_training_point)\n",
    "\n",
    "        return linear_probe_training_batch\n",
    "\n",
    "    def construct_training_dataset(self, all_training_data: list[TrainingPoint], option: str = None):\n",
    "        \"\"\"\n",
    "        Constructs final training dataset, consisting of input and expected output. \n",
    "        \"\"\"\n",
    "        all_results = []\n",
    "        for single_batch in tqdm_notebook(batch(all_training_data)):\n",
    "            current_results = self.compute_training_points_single_batch(single_batch)\n",
    "            all_results.extend(current_results)\n",
    "\n",
    "        assert len(all_results) == len(all_training_data), \"Not all training points were converted to probe inputs!\"\n",
    "\n",
    "        return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d897bdb9-3af1-4293-94c7-b4a9efede36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_training_data = successful_training_points[:5]\n",
    "\n",
    "lp_training_data_manager = LinearProbeTrainingDataManager(\n",
    "    training_data=sample_training_data, autoencoders_dict=rlhf_small,\n",
    "    model=model, tokenizer=tokenizer, target_layers = model_customizer.get_target_layers()    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d047d5e-692e-4bcd-b645-a8dbff5db40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_probe_inputs = lp_training_data_manager.construct_training_dataset(sample_training_data)\n",
    "x =[input_point.target_positive_token_id for input_point in sample_probe_inputs]\n",
    "tokenizer.decode(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7330a-8273-4c0a-8e91-b388ac543eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_dataset = lp_training_data_manager.construct_training_dataset(successful_training_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43506f8-f2c8-4294-ad1a-ff365bc0a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_training_dataset_to_wandb(training_dataset):\n",
    "    out_filename = \"training_dataset.pkl\"\n",
    "\n",
    "    with open(out_filename, \"wb\") as f_out:\n",
    "        pickle.dump(training_dataset, f_out)\n",
    "    \n",
    "    my_artifact = wandb.Artifact(\"linear_probe_training_dataset\", type=\"data\")\n",
    "    \n",
    "    # Add the list to the artifact\n",
    "    my_artifact.add_file(local_path=out_filename, name=\"linear_probe_training_dataset\")\n",
    "\n",
    "    metadata_dict = {\n",
    "        \"description\": \"Training dataset, with activations and rewards\",\n",
    "        \"source\": \"Generated by my script\",\n",
    "        \"num_examples\": len(training_dataset),\n",
    "        \"split\": \"full\"\n",
    "    }\n",
    "\n",
    "    my_artifact.metadata.update(metadata_dict)\n",
    "\n",
    "    # Log the artifact to the run\n",
    "    wandb.log_artifact(my_artifact)\n",
    "\n",
    "save_training_dataset_to_wandb(full_training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e463009c-29f8-4e13-ac0a-5be2e390f4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(full_training_dataset[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d856f0-fe75-4f9d-928f-20afdcf5a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearProbeTrainer:\n",
    "    def __init__(self, linear_probe_training_data_manager):\n",
    "        \"\"\"\n",
    "        Initialize\n",
    "        \"\"\"\n",
    "    def train_linear_probe(self):\n",
    "        \"\"\"\n",
    "        Initialize\n",
    "        \"\"\"\n",
    "\n",
    "    def assign_reward(self, training_example):\n",
    "        \"\"\"\n",
    "        Assigns reward to aver\n",
    "        \"\"\"\n",
    "\n",
    "    def compute_divergence(self, training_example):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "    def average_reward(self, token):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "\n",
    "    def average_divergence(self, token):\n",
    "        \"\"\"\n",
    "        \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
