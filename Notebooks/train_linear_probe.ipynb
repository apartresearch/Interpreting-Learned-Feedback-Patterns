{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0013fe-1e8c-463a-a0c9-dcf828e8d858",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import pprint\n",
    "import random\n",
    "\n",
    "from collections import Counter\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import numpy as np\n",
    "import wandb\n",
    "\n",
    "from scipy.sparse import csr_matrix, vstack\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdf6b59-1d37-48f1-b374-840c38ea254b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'linear_probe_training_dataset'\n",
    "policy_model_name = 'gpt_neo_125m_utility_reward'\n",
    "project_name = 'utility_reconstruction'\n",
    "version = 'v1'\n",
    "random_seed = 42\n",
    "wandb_api_key = 'YOUR_KEY_HERE'\n",
    "\n",
    "os.environ['WANDB_API_KEY'] = wandb_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee2fc8c-70b0-4c0e-ad42-13606a477ddb",
   "metadata": {},
   "source": [
    "### Randomization and other utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e67890f-786f-49b6-a141-b229052f795c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_split_list(lst, split_ratio=0.8, seed=random_seed):\n",
    "    if seed is not None:\n",
    "        random.seed(seed)\n",
    "    \n",
    "    shuffled_list = lst[:]\n",
    "    random.shuffle(shuffled_list)\n",
    "    \n",
    "    split_index = int(len(shuffled_list) * split_ratio)\n",
    "    return shuffled_list[:split_index], shuffled_list[split_index:]\n",
    "\n",
    "random_split_list([1,2,3,4,5,6,7,8,9,10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdedb5d-0f1d-41f6-893f-fb7bf972ba7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def concantenate_matrices(layer_to_csr_dict):\n",
    "    \"\"\"\n",
    "    Given a dictionary of layername_to_features matrices, this flattens and concatenates\n",
    "    the matrices, in canoncial sorted order of the dictionary keys (the layernames).\n",
    "    \"\"\"\n",
    "    sorted_matrices = [\n",
    "        layer_to_csr_dict[key] for key in sorted(layer_to_csr_dict.keys())\n",
    "    ]\n",
    "    concatenated_matrix = vstack(sorted_matrices)\n",
    "    return concatenated_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc8deb1a-299a-4852-974b-7be3315a8154",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(matrix1: csr_matrix, matrix2: csr_matrix):\n",
    "    # Convert CSR matrices to dense arrays for cdist\n",
    "    dense_matrix1 = matrix1.toarray().flatten()\n",
    "    dense_matrix2 = matrix2.toarray().flatten()\n",
    "\n",
    "    # Compute Euclidean distance using cdist\n",
    "    distance = np.linalg.norm(dense_matrix1 - dense_matrix2)\n",
    "\n",
    "    return distance\n",
    "\n",
    "def euclidean_distance_bw_dicts_of_csr_matrices(\n",
    "    matrix_dict_1: dict[str, csr_matrix], matrix_dict_2: dict[str, csr_matrix]):\n",
    "\n",
    "    feature_matrix_1 = concantenate_matrices(matrix_dict_1)\n",
    "    feature_matrix_2 = concantenate_matrices(matrix_dict_2)\n",
    "\n",
    "    return euclidean_distance(feature_matrix_1, feature_matrix_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec088ec-f34c-41a8-a008-bc8d9526d6ce",
   "metadata": {},
   "source": [
    "### Load artifact from wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b19e4f-9fd9-4886-aace-0573ad036fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "run = wandb.init(project=f'{project_name}_{policy_model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12e0f28-1948-4248-800a-ebe5d4acc53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.run.config['random_seed'] = random_seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c58ec0f-d247-4ae6-beb9-10bcd55ca416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_linear_probe_training_dataset(policy_model_name=policy_model_name, project_name=project_name, version=version):\n",
    "    artifact_path = f'linear_probe_training_dataset_{policy_model_name}:{version}'\n",
    "    \n",
    "    artifact = run.use_artifact(\n",
    "        f'nlp_and_interpretability/{project_name}/{artifact_path}', type='data'\n",
    "    )\n",
    "    artifact_dir = artifact.download()\n",
    "\n",
    "    with open(f'artifacts/{artifact_path}/{filename}', 'rb') as f_in:\n",
    "        training_dataset = pickle.load(f_in)\n",
    "\n",
    "    return training_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3702f8b9-fda0-4857-ba97-31549bbfe4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TextTokensIdsTarget:\n",
    "    attention_mask: list[int]\n",
    "    text: str\n",
    "    tokens: list[str]\n",
    "    ids: list[int]\n",
    "    target_token: str\n",
    "    target_token_id: int\n",
    "    target_token_position: int\n",
    "\n",
    "    @staticmethod\n",
    "    def get_tensorized(datapoints: \"TextTokensIdsTarget\"):\n",
    "        max_length = max([len(datapoint.tokens) for datapoint in datapoints])\n",
    "        \n",
    "        input_ids = [datapoint.ids for datapoint in datapoints]\n",
    "        attention_masks = [datapoint.attention_mask for datapoint in datapoints]\n",
    "\n",
    "        input_ids_padded = pad_list_of_lists(input_ids, tokenizer.encode(tokenizer.pad_token)[0])\n",
    "        attention_masks_padded = pad_list_of_lists(attention_masks, 0)\n",
    "        all_tokenized = {\n",
    "            \"input_ids\": torch.IntTensor(input_ids_padded).cuda(), \"attention_mask\": torch.ByteTensor(attention_masks_padded).cuda()\n",
    "        }\n",
    "        return all_tokenized\n",
    "\n",
    "\n",
    "\n",
    "### Source training point in the wandb artifact\n",
    "class TrainingPoint:\n",
    "\n",
    "    def __init__(self, input_dict: dict, tokenizer=None):\n",
    "        self.input_dict = input_dict\n",
    "        self.positive_text = input_dict['input_text']\n",
    "        self.negative_text = input_dict['output_text']\n",
    "        self.neutral_text = input_dict['neutral_text']\n",
    "        \n",
    "        # Dictionary of layer name to activations by mlp layer.\n",
    "        self.activations: dict = None\n",
    "\n",
    "        # Dictionary of layer name to autoencoder feature by mlp layer\n",
    "        self.autoencoder_feature: dict = None\n",
    "\n",
    "        # Reward value of target_token.\n",
    "        self.target_positive_reward = None\n",
    "        self.target_negative_reward = None\n",
    "\n",
    "        self.positive_text_tokens, self.positive_input_ids = get_tokens_and_ids(self.positive_text)\n",
    "        self.negative_text_tokens, self.negative_token_ids = get_tokens_and_ids(self.negative_text)\n",
    "        \n",
    "        self.positive_words = input_dict['positive_words']\n",
    "        self.negative_words = list(input_dict['new_words'].values())\n",
    "        self.neutral_words = list(input_dict['neutral_words'].values())\n",
    "\n",
    "        self.target_positive_reward = None\n",
    "        self.target_positive_token = None\n",
    "        self.target_positive_token_id = None\n",
    "    \n",
    "        self.target_negative_reward = None\n",
    "        self.target_negative_token = None\n",
    "        self.target_negative_token_id = None\n",
    "\n",
    "        self.target_neutral_token = None\n",
    "        self.target_neutral_token_id = None\n",
    "\n",
    "        try:\n",
    "            self.trimmed_positive_example: \"TextTokensIdTarget\" = trim_example(self.positive_text, self.positive_words)\n",
    "            if self.trimmed_positive_example:\n",
    "                positive_token = self.trimmed_positive_example.target_token.strip().lower()\n",
    "                self.target_positive_reward = lexicon.get(positive_token, None)\n",
    "                self.target_positive_token = positive_token\n",
    "                self.target_positive_token_id = self.trimmed_positive_example.target_token_id\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict} for positive example.')\n",
    "            self.trimmed_positive_example = None\n",
    "        \n",
    "        try:\n",
    "            self.trimmed_negative_example: \"TextTokensIdTarget\" = trim_example(self.negative_text, self.negative_words)\n",
    "            if self.trimmed_negative_example:\n",
    "                negative_token = self.trimmed_negative_example.target_token.strip().lower()\n",
    "                self.target_negative_reward = lexicon.get(negative_token, None)\n",
    "                self.target_negative_token = negative_token\n",
    "                self.target_negative_token_id = self.trimmed_negative_example.target_token_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict} for negative example.')\n",
    "            self.trimmed_negative_example = None\n",
    "\n",
    "        try:\n",
    "            self.trimmed_neutral_example: \"TextTokensIdTarget\" = trim_example(self.neutral_text, self.neutral_words)\n",
    "            if self.trimmed_neutral_example:\n",
    "                self.target_neutral_token = self.trimmed_neutral_example.target_token.strip().lower()\n",
    "                self.target_neutral_token_id = self.trimmed_neutral_example.target_token_id\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f'Caught exception {e} on {input_dict} for neutral example.')\n",
    "            self.trimmed_neutral_example = None\n",
    "\n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.__dict__)\n",
    "\n",
    "\n",
    "class LinearProbeTrainingPoint:\n",
    "    def __init__(\n",
    "        self, training_point: \"TrainingPoint\",\n",
    "        # positive token\n",
    "        target_positive_token_id: int,\n",
    "        target_positive_token: str,\n",
    "        positive_token_ae_features: [str, Tensor], \n",
    "        # negative token\n",
    "        target_negative_token_id: int,\n",
    "        target_negative_token: str,\n",
    "        negative_token_ae_features: [str, Tensor],\n",
    "        # neutral token\n",
    "        target_neutral_token_id: int,\n",
    "        target_neutral_token: str,\n",
    "        neutral_token_ae_features: [str, Tensor]\n",
    "    ):\n",
    "        self.training_point: \"TrainingPoint\" = training_point\n",
    "\n",
    "        self.target_positive_token = target_positive_token\n",
    "        self.target_positive_token_id = target_positive_token_id\n",
    "        self.target_positive_reward = self.training_point.target_positive_reward\n",
    "        self.positive_token_ae_features = positive_token_ae_features\n",
    "\n",
    "        self.target_negative_token = target_negative_token\n",
    "        self.target_negative_token_id = target_negative_token_id\n",
    "        self.target_negative_reward = self.training_point.target_negative_reward\n",
    "        self.negative_token_ae_features = negative_token_ae_features\n",
    "\n",
    "        self.target_neutral_token = target_neutral_token\n",
    "        self.target_neutral_token_id = target_neutral_token_id\n",
    "        self.neutral_token_ae_features = neutral_token_ae_features\n",
    "\n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba321f7-398e-4861-9378-17c1901f16ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_training_dataset = load_linear_probe_training_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44bfafb8-67ad-4985-90e6-31fe9ec2c513",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_split_dataset, test_split_dataset = random_split_list(full_training_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0a98c8-6453-431b-ad59-992584ffb118",
   "metadata": {},
   "source": [
    "### Define linear probe helper classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d14ff89-3dbc-4120-97ce-6a92061c03ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class LinearProbeFinalInput:\n",
    "    token: str\n",
    "    token_id: int\n",
    "    divergence: float     # Divergence of the token to neutral token\n",
    "    features: csr_matrix  # Corresponds to the features of positive or negative token\n",
    "    point_type: str    # Can be positive or negative\n",
    "\n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self.__dict__)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5f5a31-26cf-4976-b800-fe0ab12eabff",
   "metadata": {},
   "source": [
    "### Construct training dataset and linear probe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56d9a73e-a0e0-43ec-b122-0ce6429f24f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_lp_training_point_to_pair_of_lp_final_inputs(lp_training_point: LinearProbeTrainingPoint) -> list[LinearProbeFinalInput]:\n",
    "    positive_features = concantenate_matrices(\n",
    "        lp_training_point.positive_token_ae_features)\n",
    "\n",
    "    negative_features = concantenate_matrices(\n",
    "        lp_training_point.negative_token_ae_features)\n",
    "\n",
    "    neutral_features = concantenate_matrices(\n",
    "        lp_training_point.neutral_token_ae_features)\n",
    "\n",
    "    positive_token = lp_training_point.target_positive_token\n",
    "    positive_token_id = lp_training_point.target_positive_token_id\n",
    "    positive_divergence = euclidean_distance(\n",
    "        positive_features, neutral_features\n",
    "    )\n",
    "\n",
    "    # Positive input training example.\n",
    "    positive_probe_final_input = LinearProbeFinalInput(\n",
    "        token=positive_token, token_id=positive_token_id,\n",
    "        divergence=positive_divergence, features=positive_features,\n",
    "        point_type='positive'\n",
    "    )\n",
    "\n",
    "    negative_token = lp_training_point.target_negative_token\n",
    "    negative_token_id = lp_training_point.target_negative_token_id\n",
    "    negative_divergence = euclidean_distance(\n",
    "        negative_features, neutral_features\n",
    "    )\n",
    "\n",
    "    # Negative input training example - multiply divergence by minus one.\n",
    "    negative_probe_final_input = LinearProbeFinalInput(\n",
    "        token=negative_token, token_id=negative_token_id,\n",
    "        divergence=-1*negative_divergence, features=negative_features,\n",
    "        point_type='negative'\n",
    "    )\n",
    "\n",
    "    return [positive_probe_final_input, negative_probe_final_input]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe9a6e0-aa9e-419f-85e8-a654f66a205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_point = train_split_dataset[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf7c647-5493-4c6c-8676-988dad5e7c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_test_point, negative_test_point = map_lp_training_point_to_pair_of_lp_final_inputs(test_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99107626-892e-45aa-9207-95160b1e988d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\nPositive point:\\n{pprint.pformat(positive_test_point)}')\n",
    "print(f'\\nNegative point:\\n{pprint.pformat(negative_test_point)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d1c9ef-42dc-44c1-9353-bc75c7b8cba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_lp_dataset_to_final_input_dataset(\n",
    "    input_dataset: list[LinearProbeTrainingPoint]) -> list[LinearProbeFinalInput]:\n",
    "\n",
    "    final_dataset = []\n",
    "\n",
    "    for datapoint in tqdm_notebook(input_dataset):\n",
    "        positive_point, negative_point = map_lp_training_point_to_pair_of_lp_final_inputs(datapoint)\n",
    "        final_dataset.append(positive_point)\n",
    "        final_dataset.append(negative_point)\n",
    "\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea55e8a-3c01-4e1d-965b-6a28da79d813",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapped_train_split_dataset: list[LinearProbeFinalInput] = map_lp_dataset_to_final_input_dataset(\n",
    "    input_dataset = train_split_dataset\n",
    ")\n",
    "\n",
    "mapped_test_split_dataset: list[LinearProbeFinalInput] = map_lp_dataset_to_final_input_dataset(\n",
    "    input_dataset = test_split_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae3b9d7-c635-4f37-a6e2-40ff2ab02cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureConstructor:\n",
    "\n",
    "    def construct_feature_representation(self, linear_probe_inputs):\n",
    "        feature_rep = np.array([point.features.toarray().flatten() for point in linear_probe_inputs])\n",
    "        return feature_rep\n",
    "\n",
    "feature_constructor = FeatureConstructor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c574c5f-d26d-4f78-9ce2-95440a035f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_linear_model(train_linear_probe_inputs: list[LinearProbeFinalInput], feature_constructor: FeatureConstructor = feature_constructor):\n",
    "    input_points = feature_constructor.construct_feature_representation(train_linear_probe_inputs)\n",
    "\n",
    "    output_points = np.array([point.divergence for point in train_linear_probe_inputs])\n",
    "\n",
    "    print(f'Shapes are {input_points.shape} and {output_points.shape}')\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(input_points, output_points)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f77205-df75-4cbf-9aee-70e596f9c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = train_linear_model(train_linear_probe_inputs=mapped_train_split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121272b9-3d72-4b07-9313-48c8bc77d6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitted_values(linear_model, test_linear_probe_inputs, feature_constructor: FeatureConstructor = feature_constructor):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    test_inputs = feature_constructor.construct_feature_representation(test_linear_probe_inputs)\n",
    "    test_values = linear_model.predict(test_inputs)\n",
    "    return test_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d852ec-1308-4f79-99f9-cded3cfe6ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_values = get_fitted_values(linear_model=linear_model, test_linear_probe_inputs=mapped_test_split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56b1855-ae3a-4a49-8bbd-7ba08b6deea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_values_and_inputs = list(zip(fitted_values, mapped_test_split_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b471793-7c97-476f-a77d-f69a0cf50834",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_values_and_inputs[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8c8fd5-95c6-4ea0-82a3-bc8924874dcb",
   "metadata": {},
   "source": [
    "### Do analysis on divergence values viz-a-viz original Vader lexicon."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
