{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "USRzf3NLDvXN"
      },
      "outputs": [],
      "source": [
        "!pip install datasets anthropic openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import random\n",
        "import logging\n",
        "import anthropic\n",
        "import openai\n",
        "import numpy as np\n",
        "\n",
        "from getpass import getpass\n",
        "from datasets import load_dataset\n",
        "from scipy.spatial.distance import cdist"
      ],
      "metadata": {
        "id": "B0GSmWIfZds3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO)"
      ],
      "metadata": {
        "id": "KfICSlDf0K2x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANTHROPIC_API_KEY = getpass(\"ANTHROPIC_API_KEY: \")\n",
        "anthropic_client = anthropic.Anthropic(api_key=ANTHROPIC_API_KEY)\n",
        "\n",
        "OPENAI_API_KEY = getpass(\"ANTHROPIC_API_KEY: \")\n",
        "openai_client = openai.OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "ICx9ZGtKZgFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(split_percentage='validation[:1%]') -> list:\n",
        "    dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=split_percentage)\n",
        "    return [sentence.strip() for item in dataset for sentence in item['text'].split('\\n') if sentence.strip()]"
      ],
      "metadata": {
        "id": "rtYljRgmZmmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def classify_sentence(sentence: str, model_description: str) -> str:\n",
        "    \"\"\"Classify a sentence with the given model description.\"\"\"\n",
        "    prompt = (\n",
        "        f\"Given that a reward model {model_description}, classify the following sentence as 'positive' (+), \"\n",
        "        f\"'neutral' (0), or 'negative' (-):\\n\\n{sentence}\\n\\nOutput only a single token (+, 0 or -) \\\n",
        "        based on the relation of the sentence to the reward model.\"\n",
        "    )\n",
        "    try:\n",
        "        response = anthropic_client.messages.create(\n",
        "            model=\"claude-3-haiku-20240307\",\n",
        "            max_tokens=10,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        return response.content[0].text if response.content else \"Error\"\n",
        "    except Exception as e:\n",
        "        logging.error(\"Error in classify_sentence: %s\", e)\n",
        "        return \"Error\""
      ],
      "metadata": {
        "id": "B0aFATNFaSFc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_embeddings(sentences: list) -> np.array:\n",
        "    try:\n",
        "        response = openai_client.Embedding.create(input=sentences, model=\"text-embedding-3-small\")\n",
        "        return np.array([embedding['embedding'] for embedding in response['data']])\n",
        "    except Exception as e:\n",
        "        logging.error(\"Embedding error: %s\", e)\n",
        "        return None"
      ],
      "metadata": {
        "id": "tYN_uld44Je2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_triples(sentences: list, description: str, embed: bool = False) -> list:\n",
        "    sentiment_dict = {'positive': [], 'neutral': [], 'negative': []}\n",
        "    for sentence in sentences:\n",
        "        sentiment = classify_sentence(sentence, description)\n",
        "        if sentiment in sentiment_dict:\n",
        "            sentiment_dict[sentiment].append(sentence)\n",
        "\n",
        "    if embed:\n",
        "        for sentiment, sents in sentiment_dict.items():\n",
        "            if sents:\n",
        "                embeddings = get_embeddings(sents)\n",
        "                if embeddings is not None:\n",
        "                    distance_matrix = cdist(embeddings, embeddings, 'euclidean')\n",
        "                    indices = np.argmin(distance_matrix + np.eye(len(sents)) * 1e10, axis=1)\n",
        "                    sentiment_dict[sentiment] = [sents[i] for i in indices]\n",
        "\n",
        "    min_length = min(len(sentiment_dict[key]) for key in sentiment_dict)\n",
        "    return [(sentiment_dict['negative'][i], sentiment_dict['neutral'][i], sentiment_dict['positive'][i]) for i in range(min_length)]"
      ],
      "metadata": {
        "id": "NfhEvsgVaUW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def store_triples(triples: list, output_path='triples.json'):\n",
        "    with open(output_path, 'w', encoding='utf-8') as file:\n",
        "        json.dump(triples, file, ensure_ascii=False, indent=4)"
      ],
      "metadata": {
        "id": "-hLS4XNNaYCp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main(embed: bool = False):\n",
        "    sentences = load_data()\n",
        "    triples = generate_triples(sentences, REWARD_MODEL_DESCRIPTION, embed=embed)\n",
        "    store_triples(triples)"
      ],
      "metadata": {
        "id": "yxbv0h3W1FBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "REWARD_MODEL_DESCRIPTION = 'calculates reward based on how positive the sentiment of the input is'"
      ],
      "metadata": {
        "id": "NNXXlYnlwonA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main(embed=True)"
      ],
      "metadata": {
        "id": "g_nBBjQvaZgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### If you're planning on making this an installable package this might be more useful:"
      ],
      "metadata": {
        "id": "F1AxElUC6Ae9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SentenceClassifier:\n",
        "    def __init__(self, description: str, api_clients: dict):\n",
        "        self.description = description\n",
        "        self.anthropic_client = api_clients['anthropic']\n",
        "        self.openai_client = api_clients['openai']\n",
        "        self.sentiment_dict = {'positive': [], 'neutral': [], 'negative': []}\n",
        "\n",
        "    def load_data(self, split_percentage='validation[:1%]') -> list:\n",
        "        dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\", split=split_percentage)\n",
        "        return [sentence.strip() for item in dataset for sentence in item['text'].split('\\n') if sentence.strip()]\n",
        "\n",
        "    def classify_sentence(self, sentence: str) -> str:\n",
        "        prompt = (\n",
        "            f\"Given that a reward model {self.description}, classify the following sentence as 'positive' (+), \"\n",
        "            f\"'neutral' (0), or 'negative' (-):\\n\\n{sentence}\\n\\nOutput only a single token (+, 0 or -) \\\n",
        "            based on the relation of the sentence to the reward model.\"\n",
        "        )\n",
        "        response = self.anthropic_client.messages.create(model=\"claude-3-haiku-20240307\", max_tokens=10, messages=[{\"role\": \"user\", \"content\": prompt}])\n",
        "        return response.content[0].text.strip() if response.content else \"Error\"\n",
        "\n",
        "    def get_embeddings(self, sentences: list) -> np.array:\n",
        "        response = self.openai_client.Embedding.create(input=sentences, model=\"text-embedding-3-small\")\n",
        "        return np.array([embedding['embedding'] for embedding in response['data']])\n",
        "\n",
        "    def categorize_sentences(self, sentences: list):\n",
        "        for sentence in sentences:\n",
        "            sentiment = self.classify_sentence(sentence)\n",
        "            if sentiment in self.sentiment_dict:\n",
        "                self.sentiment_dict[sentiment].append(sentence)\n",
        "\n",
        "    def embed_and_sort_sentences(self):\n",
        "        for sentiment, sents in self.sentiment_dict.items():\n",
        "            if sents:\n",
        "                embeddings = self.get_embeddings(sents)\n",
        "                indices = self.get_closest_indices(embeddings)\n",
        "                self.sentiment_dict[sentiment] = [sents[i] for i in indices]\n",
        "\n",
        "    def get_closest_indices(self, embeddings: np.array) -> list:\n",
        "        distance_matrix = cdist(embeddings, embeddings, 'euclidean')\n",
        "        np.fill_diagonal(distance_matrix, np.inf)\n",
        "        return np.argmin(distance_matrix, axis=1)\n",
        "\n",
        "    def generate_triples(self, embed: bool = False) -> list:\n",
        "        if embed:\n",
        "            self.embed_and_sort_sentences()\n",
        "\n",
        "        min_length = min(len(self.sentiment_dict[key]) for key in self.sentiment_dict)\n",
        "        return [(self.sentiment_dict['negative'][i], self.sentiment_dict['neutral'][i], self.sentiment_dict['positive'][i]) for i in range(min_length)]"
      ],
      "metadata": {
        "id": "XgVOlosW6I8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "api_clients = {\n",
        "    'anthropic': anthropic.Anthropic(api_key=os.getenv(\"ANTHROPIC_API_KEY\")),\n",
        "    'openai': openai.OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
        "}\n",
        "classifier = SentenceClassifier(description='calculates reward based on how positive the sentiment of the input is', api_clients=api_clients)\n",
        "sentences = classifier.load_data()\n",
        "classifier.categorize_sentences(sentences)\n",
        "triples = classifier.generate_triples(embed=True)"
      ],
      "metadata": {
        "id": "fb1m-3Ks6M8f"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}